import os
import zipfile
import tempfile
import requests
from bs4 import BeautifulSoup
from contextlib import closing
from hwp5.hwp5html import HTMLTransform
from hwp5.xmlmodel import Hwp5File
import fitz  # PyMuPDF
import re
import pandas as pd
from docx import Document
import openpyxl
import json
import psycopg2
from urllib.parse import urlencode, quote_plus
from datetime import datetime, timedelta
import openai
from tqdm import tqdm

# í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€ê²½(í•œêµ­ ì‹œê°„ëŒ€(KST)ëŠ” UTC+9)
now_kst = datetime.utcnow() + timedelta(hours=9)

# .env ë¡œë“œ
SUPABASE_DB_URL = os.getenv("SUPABASE_DB_URL")
API_KEY = os.getenv("G2B_API_KEY")

# DB ì—°ê²°
conn = psycopg2.connect(SUPABASE_DB_URL)
cur = conn.cursor()

# Supabaseì—ì„œ ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    cur.execute("""
    SELECT raw
    FROM bids_live
    ORDER BY 
        raw->>'bidNtceDt' DESC, 
        raw->>'bidNtceBgn' DESC LIMIT 1                
""")
    result = cur.fetchone()
    raw = json.loads(result[0]) if result else {}
    start_time = raw.get("bidNtceBgn", (now_kst - timedelta(minutes=10)).strftime("%Y%m%d%H%M"))
    
except Exception as e:
    print("start_time ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜:", e)
    start_time = (now_kst - timedelta(minutes=10)).strftime("%Y%m%d%H%M")

end_time = (now_kst).strftime("%Y%m%d%H%M")

# ====== ë‚˜ë¼ì¥í„° ê³µê³ ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ======
# ë‚˜ë¼ì¥í„° API í˜¸ì¶œ
def nara_api() :
    BASE_URL = ["http://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoCnstwk",
                "http://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServc",
                "http://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoFrgcpt",
                "http://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoThng"]

    tag = ["ê³µì‚¬", "ìš©ì—­", "ì™¸ì", "ë¬¼í’ˆ"]

    page = 1
    info = []

    for t, u in zip(tag, BASE_URL) :
        while True :
            params = {
                'serviceKey': API_KEY,
                'pageNo': page,
                'numOfRows': 100,
                'inqryDiv': 1,
                'type': 'json',
                'inqryBgnDt': start_time,
                'inqryEndDt': end_time
            }
            url = f"{u}?{urlencode(params, quote_via=quote_plus)}"

            try:
                response = requests.get(url)
                data = response.json()
                items = data['response']['body']['items']
                
                if not items :
                    break
                else :
                    for item in items :
                        item["bsnsDivNm"] = t        
                        info_dict = {}
                        info_dict["bidNtceNm"] = item["bidNtceNm"]
                        info_dict["bidNtceNo"] = item["bidNtceNo"]
                        info_dict["bsnsDivNm"] = item["bsnsDivNm"]
                        info_dict["stdNtceDocUrl"] = item["stdNtceDocUrl"]
                        info_dict["bidNtceDtlUrl"] = item["bidNtceDtlUrl"]
                        info.append(info_dict)
                    page += 1
            except Exception as e:
                print(e)
    return info


# ====== ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ======
class DocumentTextExtractor:
    def __init__(self):
        pass

    @staticmethod
    def is_hwp5_file(file_path):
        try:
            with open(file_path, "rb") as f:
                header = f.read(8)
                return header.startswith(b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1')
        except:
            return False

    @staticmethod
    def is_pdf_file(file_path):
        try:
            with open(file_path, "rb") as f:
                header = f.read(4)
                return header.startswith(b"%PDF")
        except:
            return False

    @staticmethod
    def extract_text_from_hwp5(file_path):
        import sys
        class DummyFile(object):
            def write(self, x): pass
        old_stdout, old_stderr = sys.stdout, sys.stderr
        sys.stdout = DummyFile()
        sys.stderr = DummyFile()
        try:
            with closing(Hwp5File(file_path)) as hwp_file:
                transformer = HTMLTransform()
                with tempfile.TemporaryDirectory() as tmpdir:
                    transformer.transform_hwp5_to_dir(hwp_file, tmpdir)
                    html_path = os.path.join(tmpdir, "index.xhtml")
                    with open(html_path, "r", encoding="utf-8") as f:
                        soup = BeautifulSoup(f, "xml")
                    text_elements = soup.find_all(["p", "td", "th"])
                    return "\n".join(el.get_text(strip=True) for el in text_elements if el.get_text(strip=True))
        except Exception as e:
            print(f"âš ï¸ HWP5 ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ìŠ¤íƒ€ì¼ ì˜¤ë¥˜ í¬í•¨ ê°€ëŠ¥): {e}")
            return ""
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

    @staticmethod
    def extract_text_from_hwpx(file_path):
        with zipfile.ZipFile(file_path, 'r') as z:
            try:
                with z.open("Contents/section0.xml") as f:
                    soup = BeautifulSoup(f.read(), "xml")
            except KeyError:
                raise ValueError("HWPX ë³¸ë¬¸(section0.xml)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        parts = []
        paragraphs = soup.find_all("hp:p")
        parts.extend(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        table_cells = soup.find_all(["hp:cell", "hp:th"])
        parts.extend(cell.get_text(strip=True) for cell in table_cells if cell.get_text(strip=True))
        headers = soup.find_all("hp:header")
        footers = soup.find_all("hp:footer")
        parts.extend(el.get_text(strip=True) for el in headers + footers if el.get_text(strip=True))
        footnotes = soup.find_all("hp:footnote")
        parts.extend(fn.get_text(strip=True) for fn in footnotes if fn.get_text(strip=True))
        captions = soup.find_all("hp:caption")
        parts.extend(cp.get_text(strip=True) for cp in captions if cp.get_text(strip=True))
        return "\n".join(parts)

    @staticmethod
    def extract_text_from_pdf(file_path):
        extracted_text = ""
        with fitz.open(file_path) as doc:
            for page in doc:
                extracted_text += page.get_text().strip() + "\n"
        return extracted_text

    @staticmethod
    def extract_text_from_docx(file_path):
        doc = Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

    @staticmethod
    def extract_text_from_xlsx(file_path):
        wb = openpyxl.load_workbook(file_path, data_only=True)
        texts = []
        for sheet in wb.worksheets:
            for row in sheet.iter_rows():
                row_text = [str(cell.value).strip() for cell in row if cell.value is not None]
                if row_text:
                    texts.append("\t".join(row_text))
        return "\n".join(texts)

    @staticmethod
    def extract_text_from_html(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        texts = []
        for tag in soup.find_all(["p", "td", "th", "div", "span"]):
            t = tag.get_text(strip=True)
            if t:
                texts.append(t)
        return "\n".join(texts)

    def extract_text_from_zip(self, file_path):
        texts = []
        with zipfile.ZipFile(file_path, 'r') as z:
            for name in z.namelist():
                if name.endswith(('.hwp', '.hwpx', '.pdf', '.docx', '.xlsx', '.htm', '.html')):
                    with z.open(name) as extracted:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(name)[-1]) as tmpf:
                            tmpf.write(extracted.read())
                            tmpf.flush()
                            try:
                                text = self.extract_text_auto(tmpf.name)
                                texts.append(f"[{name}]\n{text}")
                            except Exception as e:
                                print(f"ZIP ë‚´ë¶€ íŒŒì¼ ì˜¤ë¥˜: {name}, {e}")
                        os.unlink(tmpf.name)
        return "\n".join(texts)

    def extract_text_auto(self, file_path):
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        ext = os.path.splitext(file_path)[-1].lower()
        if ext in [".htm", ".html"]:
            return self.extract_text_from_html(file_path)
        elif ext == ".docx":
            return self.extract_text_from_docx(file_path)
        elif ext == ".xlsx":
            return self.extract_text_from_xlsx(file_path)
        elif ext == ".zip":
            return self.extract_text_from_zip(file_path)
        elif self.is_pdf_file(file_path):
            return self.extract_text_from_pdf(file_path)
        elif zipfile.is_zipfile(file_path):
            return self.extract_text_from_hwpx(file_path)
        elif self.is_hwp5_file(file_path):
            return self.extract_text_from_hwp5(file_path)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    def extract_text_from_url(self, file_url):
        with tempfile.TemporaryDirectory() as tmpdir:
            response = requests.get(file_url)
            print("ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ í¬ê¸°:", len(response.content), "bytes", "URL:", file_url)
            response.raise_for_status()
            content_disposition = response.headers.get("Content-Disposition", "").lower()
            filename = None
            if 'filename=' in content_disposition:
                filename = content_disposition.split('filename=')[1].split(';')[0].strip('"')
            if filename:
                ext = os.path.splitext(filename)[-1].lower()
            else:
                ext = os.path.splitext(file_url)[-1].lower()
                if ext not in ['.pdf', '.hwp', '.hwpx', '.docx', '.xlsx', '.htm', '.html', '.zip']:
                    ext = '.hwp'
            file_path = os.path.join(tmpdir, "temp_file" + ext)
            with open(file_path, "wb") as f:
                f.write(response.content)
            return self.extract_text_auto(file_path)

    @staticmethod
    def preprocess_text(raw_text):
        lines = raw_text.splitlines()
        deduped_lines = []
        prev = None
        for line in lines:
            clean = line.strip()
            if clean and clean != prev:
                deduped_lines.append(clean)
                prev = clean
        joined_text = "\n".join(deduped_lines)
        joined_text = re.sub(r"[\uf000-\uf8ff]", "", joined_text)
        chunks = re.split(r"\n{2,}|(?<=\.)\n", joined_text)
        chunks = [chunk.strip() for chunk in chunks if len(chunk.strip()) > 20]
        return chunks

    @staticmethod
    def refine_chunks_to_sentences(chunks):
        refined = []
        for chunk in chunks:
            chunk = re.sub(r"^\s*[-â€¢Â·âˆ™â—â—‹â€»]?\s*", "", chunk)
            chunk = re.sub(r"(?P<key>[ê°€-í£a-zA-Z0-9]+)\s*[:ï¼š]\s*(?P<val>.+)", r"\g<key>ì€(ëŠ”) \g<val>ì…ë‹ˆë‹¤.", chunk)
            chunk = chunk.strip()
            if chunk and not chunk.endswith(("ë‹¤", ".", "ìš”")):
                chunk += "ì…ë‹ˆë‹¤."
            refined.append(chunk)
        return refined

    def process_info_list(self, info):
        table = []
        for i in info:
            table_dict = {}
            url = i.get("stdNtceDocUrl", "").strip()
            re_text = ""
            try:
                if url and url.startswith("http"):
                    raw_text = self.extract_text_from_url(url)
                    pre_text = self.preprocess_text(raw_text)
                    re_text = self.refine_chunks_to_sentences(pre_text)
                elif url == "":
                    re_text = ""
                else:
                    print("ğŸ“› ìœ íš¨í•˜ì§€ ì•Šì€ URL:", url, i.get("bidNtceDtlUrl", ""))
            except Exception as e:
                print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e, url, i.get("bidNtceDtlUrl", ""))
            table_dict["bidNtceNo"] = i.get("bidNtceNo")
            table_dict["bidNtceNm"] = i.get("bidNtceNm")
            table_dict["bsnsDivNm"] = i.get("bsnsDivNm")
            table_dict["contents"] = re_text
            table.append(table_dict)
        return pd.DataFrame(table)
        # return table

# ====== í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ì •ë¦¬ ======

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥ ê°€ëŠ¥)
openai.api_key = os.getenv("OPENAI_API_KEY")

# ë²¡í„° ì„ë² ë”© í•¨ìˆ˜ (OpenAI text-embedding-3-small ëª¨ë¸ ì‚¬ìš©)
def get_embedding(text):
    res = openai.Embedding.create(input=[text], model="text-embedding-3-small")
    return res["data"][0]["embedding"]

# í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”
extractor = DocumentTextExtractor()

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
info = nara_api()
df = extractor.process_info_list(info)

# ======GPTê¸°ë°˜ QAìƒì„±ê¸°======
def generate_qa_gpt(chunk):
    prompt = f"""
ë‹¹ì‹ ì€ ë‚˜ë¼ì¥í„° ì…ì°° ê³µê³ ë¬¸ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì •ë¶€ ì¡°ë‹¬ ë° ê³µê³µê¸°ê´€ ì…ì°°ì— ëŒ€í•œ ê¹Šì€ ì „ë¬¸ ì§€ì‹ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°,
ë³µì¡í•œ ê³µê³ ë¬¸ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì‚¬ì—…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•µì‹¬ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ì–´ì§„ "ë¬¸ë‹¨"ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:
1.  ë¬¸ë‹¨ì´ ë¬´ìŠ¨ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” 'keyword'ë¥¼ ë‹¨ì–´ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
2.  ë¬¸ë‹¨ì˜ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬, ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ ë‹¤ì–‘í•œ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ ìŒë“¤ì„ 'qa_pairs' ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    - ê° ì§ˆë¬¸ì€ ë¬¸ë‹¨ ë‚´ì˜ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê° ë‹µë³€ì€ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ë‹¨ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
    - ë¬¸ë‹¨ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´, ê°€ëŠ¥í•œ í•œ ë§ì€ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ë¬¸ë‹¨: "{chunk}"

**âš ï¸ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ ë“±ì˜ ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
**âš ï¸ JSON ë¸”ë¡ì„ ê°ì‹¸ëŠ” ```json ë“±ì˜ Markdown ì½”ë“œë¸”ë¡ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.**


JSON í˜•ì‹ ì˜ˆì‹œ:
{{
  "keyword": "ì£¼ì œì–´",
  "qa_pairs": [
    {{
      "question": "ì²« ë²ˆì§¸ ì˜ˆìƒ ì§ˆë¬¸",
      "answer": "ì²« ë²ˆì§¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"
    }},
    {{
      "question": "ë‘ ë²ˆì§¸ ì˜ˆìƒ ì§ˆë¬¸",
      "answer": "ë‘ ë²ˆì§¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"
    }}
  ]
}}
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    
    # ì•ˆì „í•œ JSON íŒŒì‹± (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬)
    content = response['choices'][0]['message']['content']
    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨\nì‘ë‹µ:", content)
        raise e

# DB ì—°ê²°
chunk_id_counter = 1  # ë¬¸ë‹¨ ê¸°ì¤€ ID ìˆ˜ë™ ì¦ê°€

for _, row in tqdm(df.iterrows(), total=len(df)):
    sentence = row["contents"]
    qa_result = generate_qa_gpt(sentence)
    keyword = qa_result["keyword"]
    qa_pairs = qa_result["qa_pairs"]

    metadata_base = {
        "chunk_id": chunk_id_counter,
        "bidNtceNo": row.get("bidNtceNo", "unknown"),
        "bidNtceNm": row.get("bidNtceNm", "unknown"),
        "bsnsDivNm": row.get("bsnsDivNm", "unknown")
    }

    # sentence ì €ì¥
    s_embedding = get_embedding(sentence)
    cur.execute("""
        INSERT INTO semantic_chunks (type, text, embedding, metadata)
        VALUES (%s, %s, %s, %s)
    """, ("sentence", sentence, s_embedding, json.dumps(metadata_base)))

    # keyword ì €ì¥
    k_embedding = get_embedding(keyword)
    cur.execute("""
        INSERT INTO semantic_chunks (type, text, embedding, metadata)
        VALUES (%s, %s, %s, %s)
    """, ("keyword", keyword, k_embedding, json.dumps(metadata_base)))

    # question/answer ì €ì¥
    for qa in qa_pairs:
        q_embedding = get_embedding(qa["question"])
        a_embedding = get_embedding(qa["answer"])

        cur.execute("""
            INSERT INTO semantic_chunks (type, text, embedding, metadata)
            VALUES (%s, %s, %s, %s)
        """, ("question", qa["question"], q_embedding, json.dumps(metadata_base)))

        cur.execute("""
            INSERT INTO semantic_chunks (type, text, embedding, metadata)
            VALUES (%s, %s, %s, %s)
        """, ("answer", qa["answer"], a_embedding, json.dumps(metadata_base)))

    chunk_id_counter += 1

conn.commit()
cur.close()
conn.close()