import streamlit as st
import pandas as pd
from supabase import create_client, Client
from streamlit_autorefresh import st_autorefresh
from datetime import datetime, timedelta
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI
import numpy as np
from typing import List, Dict, Tuple, TypedDict, Optional, Annotated
import operator
import time

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import Union

# LangChain imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# API ë° ì›¹ ìš”ì²­ ê´€ë ¨
from urllib.parse import urlencode, quote_plus
import requests

# ========== ì´ˆê¸° ì„¤ì • ë° API í‚¤ ==========
# API í‚¤ ë° ì„¤ì •ê°’ ì´ˆê¸°í™”
OPENAI_API_KEY = ""  # OpenAI API í‚¤
SUPABASE_URL = ""    # ì˜ˆ: "https://your-project.supabase.co"
SUPABASE_ANON_KEY = ""  # Supabase Anonymous Key
SUPABASE_SERVICE_ROLE_KEY = ""  # Supabase Service Role Key (ê´€ë¦¬ì ê¶Œí•œ)

# ë‚˜ë¼ì¥í„° API í‚¤
NARATANG_API_KEY = "6FAWdycqkHj1fAb/TpeNQLlEzjIB+7eozDneMjTwZPUWDmva0FamSPT1uGtzrxVKuub/vADLVft2bCZ+hkL5YA=="

# ========== Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ==========
@st.cache_resource
def init_supabase_client():
    """Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        st.error("Supabase URLê³¼ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return None
    
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
        return supabase
    except Exception as e:
        st.error(f"Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

# ========== ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ==========
@st.cache_resource
def init_embeddings():
    """OpenAI ì„ë² ë”© ì´ˆê¸°í™”"""
    if not OPENAI_API_KEY:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ë² ë”© ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=OPENAI_API_KEY
    )

@st.cache_resource
def init_vector_store():
    """Supabase ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
    supabase = init_supabase_client()
    embeddings = init_embeddings()
    
    if not supabase or not embeddings:
        return None
    
    try:
        vector_store = SupabaseVectorStore(
            client=supabase,
            embedding=embeddings,
            table_name="semantic_chunks",
            query_name="match_documents"
        )
        return vector_store
    except Exception as e:
        st.error(f"ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ========== OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ==========
@st.cache_resource
def init_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if not OPENAI_API_KEY:
        st.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        return None
    
    return OpenAI(api_key=OPENAI_API_KEY)

# ========== LangChain ChatOpenAI ì´ˆê¸°í™” ==========
@st.cache_resource
def init_langchain_llm():
    """LangChain ChatOpenAI ì´ˆê¸°í™”"""
    if not OPENAI_API_KEY:
        return None
    
    return ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        api_key=OPENAI_API_KEY
    )

# ========== ë°ì´í„° ì•¡ì„¸ìŠ¤ í•¨ìˆ˜ë“¤ ==========
@st.cache_data(ttl=300)  # 5ë¶„ ìºì‹œ
def get_live_bids_data():
    """ì‹¤ì‹œê°„ ì…ì°° ê³µê³  ë°ì´í„° ì¡°íšŒ"""
    supabase = init_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        # ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ ì¡°íšŒ (raw JSONBì—ì„œ bidNtceDate ì¶”ì¶œ)
        thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
        
        response = supabase.table('bids_live').select(
            "bidNtceNo, raw, created_at, updated_at"
        ).gte('raw->>bidNtceDate', thirty_days_ago).order('raw->>bidNtceDate', desc=True).execute()
        
        if response.data:
            # JSONB ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            processed_data = []
            for item in response.data:
                bid_data = {
                    'bidNtceNo': item['bidNtceNo'],
                    **item['raw']  # raw JSONB ë°ì´í„°ë¥¼ í¼ì³ì„œ ì¶”ê°€
                }
                processed_data.append(bid_data)
            
            df = pd.DataFrame(processed_data)
            return df
        else:
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"ì…ì°° ê³µê³  ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=600)  # 10ë¶„ ìºì‹œ
def get_bid_summary(bid_no: str):
    """íŠ¹ì • ê³µê³ ì˜ GPT ìš”ì•½ ì¡°íšŒ"""
    supabase = init_supabase_client()
    if not supabase:
        return "ìš”ì•½ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None, None
    
    try:
        # hwp_based ìš°ì„ , ì—†ìœ¼ë©´ basic_info
        response = supabase.table('bid_summaries').select(
            "summary, summary_type, created_at"
        ).eq('bidNtceNo', bid_no).order('summary_type', desc=True).limit(1).execute()
        
        if response.data:
            summary_doc = response.data[0]
            summary = summary_doc.get('summary', 'ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.')
            summary_type = summary_doc.get('summary_type', 'unknown')
            created_at = summary_doc.get('created_at', '')
            
            if created_at:
                created_at = pd.to_datetime(created_at).strftime("%Y-%m-%d %H:%M")
            
            return summary, created_at, summary_type
        else:
            return "ì´ ê³µê³ ì— ëŒ€í•œ ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None
            
    except Exception as e:
        return f"ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", None, None

def search_bids_by_text(query: str, limit: int = 50):
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì…ì°° ê³µê³  ê²€ìƒ‰ (JSONB ì»¬ëŸ¼ ê²€ìƒ‰)"""
    supabase = init_supabase_client()
    if not supabase:
        return []
    
    try:
        # PostgreSQL JSONB ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        response = supabase.table('bids_live').select("bidNtceNo, raw").or_(
            f"raw->>bidNtceNm.ilike.%{query}%,"
            f"raw->>ntceInsttNm.ilike.%{query}%,"
            f"raw->>dmndInsttNm.ilike.%{query}%,"
            f"raw->>bidprcPsblIndstrytyNm.ilike.%{query}%"
        ).limit(limit).execute()
        
        # ê²°ê³¼ ì²˜ë¦¬: JSONB raw ë°ì´í„°ë¥¼ í¼ì³ì„œ ë°˜í™˜
        processed_results = []
        if response.data:
            for item in response.data:
                bid_data = {
                    'bidNtceNo': item['bidNtceNo'],
                    **item['raw']  # raw JSONB ë°ì´í„°ë¥¼ í¼ì³ì„œ ì¶”ê°€
                }
                processed_results.append(bid_data)
        
        return processed_results
        
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def get_bid_detail_by_no(bid_no: str):
    """ê³µê³ ë²ˆí˜¸ë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    supabase = init_supabase_client()
    if not supabase:
        return None
    
    try:
        response = supabase.table('bids_live').select(
            "bidNtceNo, raw, created_at, updated_at"
        ).eq('bidNtceNo', bid_no).single().execute()
        
        if response.data:
            # JSONB raw ë°ì´í„°ë¥¼ í¼ì³ì„œ ë°˜í™˜
            bid_data = {
                'bidNtceNo': response.data['bidNtceNo'],
                'created_at': response.data.get('created_at'),
                'updated_at': response.data.get('updated_at'),
                **response.data['raw']  # raw JSONB ë°ì´í„°ë¥¼ í¼ì³ì„œ ì¶”ê°€
            }
            return bid_data
        else:
            return None
            
    except Exception as e:
        st.error(f"ê³µê³  ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

def search_semantic_chunks(query: str, similarity_threshold: float = 0.3, limit: int = 30):
    """ì‹œë§¨í‹± ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰)"""
    vector_store = init_vector_store()
    if not vector_store:
        return []
    
    try:
        # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        docs = vector_store.similarity_search_with_score(query, k=limit)
        
        results = []
        for doc, score in docs:
            similarity = 1 / (1 + score)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            
            if similarity >= similarity_threshold:
                results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity": similarity,
                    "bidNtceNo": doc.metadata.get('ê³µê³ ë²ˆí˜¸', 'N/A'),
                    "bidNtceNm": doc.metadata.get('ê³µê³ ëª…', 'N/A'),
                    "ntceInsttNm": doc.metadata.get('ê¸°ê´€ëª…', 'N/A')
                })
        
        return results
        
    except Exception as e:
        st.error(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ==========
def convert_to_won_format(amount):
    """ê¸ˆì•¡ì„ í•œêµ­ ì›í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        if not amount or pd.isna(amount):
            return "ê³µê³  ì°¸ì¡°"
        
        amount = float(str(amount).replace(",", ""))

        if amount >= 100000000:  # 1ì–µ ì´ìƒ
            amount_in_100m = amount / 100000000
            return f"{amount_in_100m:.1f}ì–µì›"
        elif amount >= 10000:  # 1ë§Œ ì´ìƒ
            amount_in_10k = amount / 10000
            return f"{amount_in_10k:.1f}ë§Œì›"
        else:
            return f"{int(amount):,}ì›"
        
    except Exception as e:
        return "ê³µê³  ì°¸ì¡°"

def format_won(amount):
    """ê¸ˆì•¡ í¬ë§·íŒ…"""
    try:
        if isinstance(amount, str):
            amount = amount.replace(",", "")
        amount = int(float(amount))
        return f"{amount:,}ì›"
    except (ValueError, TypeError):
        return "ê³µê³  ì°¸ì¡°"

def format_joint_contract(value):
    """ê³µë™ìˆ˜ê¸‰ ì •ë³´ í¬ë§·íŒ…"""
    if value and str(value).strip():
        return f"í—ˆìš© [{str(value).strip()}]"
    return "ê³µê³ ì„œ ì°¸ì¡°"

# ========== ë‚˜ë¼ì¥í„° API ì„¤ì • ==========
BASE_URL_COMMON = "http://apis.data.go.kr/1230000/ad/BidPublicInfoService"
API_ENDPOINTS = {
    "ê³µì‚¬": f"{BASE_URL_COMMON}/getBidPblancListInfoCnstwk",
    "ìš©ì—­": f"{BASE_URL_COMMON}/getBidPblancListInfoServc", 
    "ë¬¼í’ˆ": f"{BASE_URL_COMMON}/getBidPblancListInfoThng",
    "ì™¸ì": f"{BASE_URL_COMMON}/getBidPblancListInfoFrgcpt",
}

# ========== ì±—ë´‡ í´ë˜ìŠ¤ (Supabase ë²„ì „) ==========
class BidSearchChatbot:
    """ì…ì°° ê³µê³  ê²€ìƒ‰ ì±—ë´‡ í´ë˜ìŠ¤ - Supabase ë²„ì „"""
    
    def __init__(self):
        """ì±—ë´‡ ì´ˆê¸°í™”"""
        self.supabase = init_supabase_client()
        self.vector_store = init_vector_store()
        self.openai_client = init_openai_client()
        self.embeddings = init_embeddings()

    def search_vector_db(self, query: str, n_results: int = 10) -> List[Dict]:
        """ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.vector_store:
            st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            results = self.vector_store.similarity_search_with_score(query, k=n_results)
            
            documents = []
            for doc, score in results:
                # ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼ë¥¼ Supabaseì—ì„œ ìƒì„¸ ì •ë³´ì™€ ë§¤ì¹­
                bid_no = doc.metadata.get('ê³µê³ ë²ˆí˜¸', '')
                if bid_no:
                    bid_detail = get_bid_detail_by_no(bid_no)
                    if bid_detail:
                        document = {
                            "content": doc.page_content,
                            "metadata": {**doc.metadata, **bid_detail},
                            "score": 1 / (1 + score),  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                            "bidNtceNo": bid_no
                        }
                        documents.append(document)
            
            return documents
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_supabase_text(self, query: str, n_results: int = 10) -> List[Dict]:
        """Supabaseì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.supabase:
            return []
        
        try:
            # JSONB í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            results = search_bids_by_text(query, limit=n_results)
            
            documents = []
            for item in results:
                document = {
                    "content": f"ê³µê³ ëª…: {item.get('bidNtceNm', '')}\nê¸°ê´€: {item.get('ntceInsttNm', '')}\në¶„ë¥˜: {item.get('bsnsDivNm', '')}",
                    "metadata": item,
                    "score": 0.8,  # í…ìŠ¤íŠ¸ ë§¤ì¹­ ê¸°ë³¸ ì ìˆ˜
                    "bidNtceNo": item.get('bidNtceNo', '')
                }
                documents.append(document)
            
            return documents
        except Exception as e:
            st.error(f"Supabase í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def get_combined_search_results(self, query: str, n_results: int = 10) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²°í•©"""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.search_vector_db(query, n_results//2)
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text_results = self.search_supabase_text(query, n_results//2)
        
        # ê²°ê³¼ ê²°í•© ë° ì¤‘ë³µ ì œê±°
        combined_dict = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for doc in vector_results:
            bid_no = doc.get('bidNtceNo', '')
            if bid_no:
                combined_dict[bid_no] = doc
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µë˜ë©´ ì ìˆ˜ ì¦ê°€)
        for doc in text_results:
            bid_no = doc.get('bidNtceNo', '')
            if bid_no:
                if bid_no in combined_dict:
                    combined_dict[bid_no]['score'] += 0.3  # ì¤‘ë³µ ë°œê²¬ ì‹œ ì ìˆ˜ ì¦ê°€
                    combined_dict[bid_no]['source'] = 'vector+text'
                else:
                    doc['source'] = 'text'
                    combined_dict[bid_no] = doc
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results = list(combined_dict.values())
        results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return results[:n_results]

    def get_simple_response(self, question: str, search_results: List[Dict]) -> str:
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±"""
        if search_results:
            # ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì„ íƒ
            best_result = search_results[0]
            metadata = best_result.get("metadata", {})
            bid_name = metadata.get("bidNtceNm", "ê³µê³ ëª… ì—†ìŒ")
            org_name = metadata.get("ntceInsttNm", "ê¸°ê´€ëª… ì—†ìŒ")
            return f"ì§ˆë¬¸: {question}\n\nê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê³µê³ :\n- ê³µê³ ëª…: {bid_name}\n- ê¸°ê´€: {org_name}"
        else:
            return "ê´€ë ¨ëœ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def get_gpt_response(self, question: str, search_results: List[Dict]) -> str:
        """GPTë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        if not search_results:
            return "ê´€ë ¨ëœ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if not self.openai_client:
            return self.get_simple_response(question, search_results)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(search_results[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            metadata = doc.get('metadata', {})
            context_parts.append(f"""
        [ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {doc.get('score', 0):.2f})
        ê³µê³ ë²ˆí˜¸: {metadata.get('bidNtceNo', 'N/A')}
        ê³µê³ ëª…: {metadata.get('bidNtceNm', 'N/A')}
        ê¸°ê´€ëª…: {metadata.get('ntceInsttNm', 'N/A')}
        ë¶„ë¥˜: {metadata.get('bsnsDivNm', 'N/A')}
        ê¸ˆì•¡: {convert_to_won_format(metadata.get('asignBdgtAmt', 0))}
        ë‚´ìš©: {doc.get('content', '')[:300]}...
        """)
        
        context = "\n".join(context_parts)
        
        system_prompt = """ë‹¹ì‹ ì€ ê³µê³µì…ì°° ì •ë³´ ê²€ìƒ‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        ë‹µë³€ ì§€ì¹¨:
        1. ê²€ìƒ‰ëœ ê³µê³ ë“¤ ì¤‘ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì–¸ê¸‰
        2. ê° ê³µê³ ì˜ í•µì‹¬ ì •ë³´(ê³µê³ ëª…, ê¸°ê´€, ë§ˆê°ì¼ ë“±)ë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬
        3. ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ì´ ìˆë‹¤ë©´ ì•ˆë‚´
        4. ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆ
        """
        
        user_prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ê´€ë ¨ ê³µê³ :
{context}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ========== ì‹œë§¨í‹± ê²€ìƒ‰ í•¨ìˆ˜ë“¤ (Supabase ë²„ì „) ==========
def semantic_search_supabase(query: str, k: int = 10) -> List[Tuple[Dict, float]]:
    """
    Supabase ë²¡í„° DBì—ì„œ ì‹œë§¨í‹± ê²€ìƒ‰ ìˆ˜í–‰
    """
    vector_store = init_vector_store()
    if not vector_store:
        st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        results = vector_store.similarity_search_with_score(query, k=k)
        
        # ê²°ê³¼ ì •ë¦¬ ë° Supabaseì—ì„œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        formatted_results = []
        for doc, score in results:
            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ê³µê³ ë²ˆí˜¸ ì¶”ì¶œ
            bid_no = doc.metadata.get('ê³µê³ ë²ˆí˜¸', '')
            
            if bid_no:
                # Supabaseì—ì„œ í•´ë‹¹ ê³µê³ ì˜ ì „ì²´ ì •ë³´ ì¡°íšŒ
                bid_detail = get_bid_detail_by_no(bid_no)
                if bid_detail:
                    # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
                    similarity_score = 1 / (1 + score)
                    
                    # ë©”íƒ€ë°ì´í„° ê²°í•©
                    combined_metadata = {**doc.metadata, **bid_detail}
                    formatted_results.append((combined_metadata, similarity_score))
        
        return formatted_results
    except Exception as e:
        st.error(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def generate_rag_response_supabase(query: str, context_docs: List[Tuple[Dict, float]]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„± - Supabase ë²„ì „"""
    
    openai_client = init_openai_client()
    if not openai_client:
        return "OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, (metadata, score) in enumerate(context_docs[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        context_parts.append(f"""
    [ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {score:.2f})
    ê³µê³ ë²ˆí˜¸: {metadata.get('bidNtceNo', 'N/A')}
    ê³µê³ ëª…: {metadata.get('bidNtceNm', 'N/A')}
    ê¸°ê´€ëª…: {metadata.get('ntceInsttNm', 'N/A')}
    ë¶„ë¥˜: {metadata.get('bsnsDivNm', 'N/A')}
    ê¸ˆì•¡: {convert_to_won_format(metadata.get('asignBdgtAmt', 0))}
    ë§ˆê°ì¼: {metadata.get('bidClseDate', 'N/A')}
""")
    
    context = "\n".join(context_parts)
    
    system_prompt = """ë‹¹ì‹ ì€ ê³µê³µì…ì°° ì •ë³´ ê²€ìƒ‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

    ë‹µë³€ ì§€ì¹¨:
    1. ê²€ìƒ‰ëœ ê³µê³ ë“¤ ì¤‘ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì–¸ê¸‰
    2. ê° ê³µê³ ì˜ í•µì‹¬ ì •ë³´(ê³µê³ ëª…, ê¸°ê´€, ë§ˆê°ì¼ ë“±)ë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬
    3. ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ì´ ìˆë‹¤ë©´ ì•ˆë‚´
    4. ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆ
    """
    
    user_prompt = f"""
    ì‚¬ìš©ì ì§ˆë¬¸: {query}

    ê²€ìƒ‰ëœ ê´€ë ¨ ê³µê³ :
    {context}

    ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    """
        
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ========== ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ (ìºì‹œ ì ìš©) ==========
@st.cache_resource
def init_chatbot():
    """ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìºì‹±"""
    return BidSearchChatbot()

@st.cache_resource
def init_resources():
    """ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” - Supabase ë²„ì „"""
    # Supabase ì—°ê²°
    supabase = init_supabase_client()
    
    # ë²¡í„° DB ì´ˆê¸°í™”
    vector_store = init_vector_store()
    
    # LangChain LLM
    llm = init_langchain_llm()
    
    return supabase, vector_store, llm

# ========== ì±—ë´‡ ê¸°ëŠ¥ ==========
def process_question(question: str, chatbot: BidSearchChatbot):
    """ì§ˆë¬¸ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.chat_messages.append({"role": "user", "content": question})
    
    # ì‘ë‹µ ìƒì„±
    with st.spinner("ì…ì°° ê³µê³ ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í…ìŠ¤íŠ¸)
        search_results = chatbot.get_combined_search_results(question)
        
        # GPT ì‘ë‹µ ë˜ëŠ” ê°„ë‹¨í•œ ì‘ë‹µ
        if OPENAI_API_KEY:
            response = chatbot.get_gpt_response(question, search_results)
        else:
            response = chatbot.get_simple_response(question, search_results)
    
    # ì‘ë‹µ í‘œì‹œ
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.chat_messages.append({"role": "assistant", "content": response})

    # ========== LangGraph ìƒíƒœ ì •ì˜ (Supabase ë²„ì „) ==========
class BidSearchState(TypedDict):
    """ì…ì°° ê³µê³  ê²€ìƒ‰ ìƒíƒœ - Supabase ë²„ì „"""
    query: str
    search_type: str  # 'semantic' or 'keyword'
    category_filter: Optional[List[str]]
    date_range: Optional[Tuple[datetime, datetime]]
    supabase_results: List[Dict]  # MongoDB -> Supabase
    vector_results: List[Dict]
    combined_results: List[Dict]
    final_answer: str
    error: Optional[str]
    status_messages: Annotated[List[str], operator.add]
    quality_score: float

class HybridSearchState(TypedDict):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ - Supabase ë²„ì „"""
    query: str
    search_method: List[str]  # ì‚¬ìš©ëœ ê²€ìƒ‰ ë°©ë²•ë“¤
    supabase_results: List[Dict]  # MongoDB -> Supabase
    vector_results: List[Dict]
    api_results: Dict[str, List[Dict]]
    combined_results: List[Dict]
    final_results: List[Dict]
    summary: str
    messages: Annotated[List[Union[HumanMessage, AIMessage]], operator.add]
    error: Optional[str]
    total_count: int
    need_api_search: bool

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ (Supabase ë²„ì „) ==========
def preprocess_query_node(state: BidSearchState) -> BidSearchState:
    """ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë…¸ë“œ"""
    try:
        # ì¿¼ë¦¬ í™•ì¥ ë° ë™ì˜ì–´ ì²˜ë¦¬
        query = state["query"].lower()
        
        # ì…ì°° ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        keyword_expansions = {
            "ai": ["ì¸ê³µì§€ëŠ¥", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹"],
            "ì„œë²„": ["ì„œë²„", "ì„œë²„êµ¬ì¶•", "ì„œë²„ì‹œìŠ¤í…œ", "ì¸í”„ë¼"],
            "sw": ["ì†Œí”„íŠ¸ì›¨ì–´", "SW", "S/W", "í”„ë¡œê·¸ë¨"],
            "hw": ["í•˜ë“œì›¨ì–´", "HW", "H/W", "ì¥ë¹„"],
            "ì‹œìŠ¤í…œ": ["ì‹œìŠ¤í…œ", "ì‹œìŠ¤í…œêµ¬ì¶•", "ì •ë³´ì‹œìŠ¤í…œ"],
            "ê°œë°œ": ["ê°œë°œ", "êµ¬ì¶•", "ì œì‘", "ê°œë°œì‚¬ì—…"]
        }
        
        expanded_terms = [query]
        for key, synonyms in keyword_expansions.items():
            if key in query:
                expanded_terms.extend(synonyms)
        
        state["expanded_query"] = " ".join(set(expanded_terms))
        state["status_messages"] = [f"âœ… ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(expanded_terms)}ê°œ ê²€ìƒ‰ì–´"]
        
    except Exception as e:
        state["error"] = f"ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
        
    return state

def search_supabase_node(state: BidSearchState) -> BidSearchState:
    """Supabase ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        supabase = init_supabase_client()
        if not supabase:
            state["error"] = "Supabase ì—°ê²° ì‹¤íŒ¨"
            state["supabase_results"] = []
            return state
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_text = state.get("expanded_query", state["query"])
        
        # ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ ê²€ìƒ‰
        from datetime import datetime, timedelta
        date_30_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
        
        # JSONB í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        response = supabase.table('bids_live').select("bidNtceNo, raw").or_(
            f"raw->>bidNtceNm.ilike.%{query_text}%,"
            f"raw->>ntceInsttNm.ilike.%{query_text}%,"
            f"raw->>dmndInsttNm.ilike.%{query_text}%,"
            f"raw->>bidprcPsblIndstrytyNm.ilike.%{query_text}%"
        ).gte('raw->>bidNtceDate', date_30_days_ago).limit(50).execute()
        
        # ê²°ê³¼ ì²˜ë¦¬
        results = []
        if response.data:
            for item in response.data:
                bid_data = {
                    'bidNtceNo': item['bidNtceNo'],
                    **item['raw']  # raw JSONB ë°ì´í„°ë¥¼ í¼ì³ì„œ ì¶”ê°€
                }
                results.append(bid_data)
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if state.get("category_filter"):
            results = [r for r in results if r.get('bsnsDivNm') in state["category_filter"]]
        
        # ë‚ ì§œ í•„í„°
        if state.get("date_range"):
            start_date, end_date = state["date_range"]
            filtered_results = []
            for r in results:
                bid_date = r.get('bidNtceDate', '')
                if bid_date and start_date.strftime('%Y%m%d') <= bid_date <= end_date.strftime('%Y%m%d'):
                    filtered_results.append(r)
            results = filtered_results
        
        state["supabase_results"] = results
        state["status_messages"] = [f"âœ… Supabaseì—ì„œ {len(results)}ê°œ ê³µê³  ê²€ìƒ‰ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"Supabase ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["supabase_results"] = []
        
    return state

def search_vector_db_node(state: BidSearchState) -> BidSearchState:
    """ë²¡í„° DB ì‹œë§¨í‹± ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        vector_store = init_vector_store()
        if not vector_store:
            state["error"] = "ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨"
            state["vector_results"] = []
            return state
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        results = vector_store.similarity_search_with_score(
            state["query"], 
            k=30
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        vector_results = []
        for doc, score in results:
            metadata = doc.metadata
            similarity_score = 1 / (1 + score)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            
            # Supabaseì—ì„œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            bid_no = metadata.get('ê³µê³ ë²ˆí˜¸', '')
            if bid_no:
                bid_detail = get_bid_detail_by_no(bid_no)
                if bid_detail:
                    vector_results.append({
                        "content": doc.page_content,
                        "metadata": {**metadata, **bid_detail},
                        "similarity": similarity_score,
                        "bidNtceNo": bid_no,
                        "bidNtceNm": bid_detail.get('bidNtceNm', metadata.get('ê³µê³ ëª…', 'N/A')),
                        "ntceInsttNm": bid_detail.get('ntceInsttNm', metadata.get('ê¸°ê´€ëª…', 'N/A'))
                    })
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        filtered_results = [r for r in vector_results if r["similarity"] >= 0.3]
        
        state["vector_results"] = filtered_results
        state["status_messages"] = [f"âœ… ë²¡í„° DBì—ì„œ {len(filtered_results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["vector_results"] = []
        
    return state

def combine_results_node(state: BidSearchState) -> BidSearchState:
    """ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë…¸ë“œ"""
    try:
        supabase_results = state.get("supabase_results", [])
        vector_results = state.get("vector_results", [])
        combined_dict = {}

        # Supabase ê²°ê³¼ ì¶”ê°€
        for supabase_item in supabase_results:
            bid_no = supabase_item.get("bidNtceNo")
            if bid_no:
                combined_dict[bid_no] = {
                    **supabase_item,
                    "source": "supabase",
                    "relevance_score": 0.5
                }

        # ë²¡í„° ê²°ê³¼ ì¶”ê°€/ë³‘í•©
        for vector_item in vector_results:
            bid_no = vector_item.get("bidNtceNo")
            if bid_no:
                if bid_no in combined_dict:
                    combined_dict[bid_no]["relevance_score"] += vector_item["similarity"]
                    combined_dict[bid_no]["vector_content"] = vector_item["content"]
                    combined_dict[bid_no]["similarity"] = vector_item["similarity"]
                    combined_dict[bid_no]["source"] = "supabase+vector"
                else:
                    combined_dict[bid_no] = {
                        **vector_item["metadata"],
                        "source": "vector",
                        "relevance_score": vector_item["similarity"],
                        "vector_content": vector_item["content"],
                        "similarity": vector_item["similarity"]
                    }

        combined_results = list(combined_dict.values())
        combined_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        state["combined_results"] = combined_results[:20]

        # ê²€ìƒ‰ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        if combined_results:
            avg_score = np.mean([r["relevance_score"] for r in combined_results[:10]])
            state["quality_score"] = round(float(avg_score), 3)
        else:
            state["quality_score"] = 0.0

        state["status_messages"] = [f"âœ… {len(state['combined_results'])}ê°œ ê³µê³ ë¡œ í†µí•© ì™„ë£Œ"]

    except Exception as e:
        state["error"] = f"ê²°ê³¼ í†µí•© ì˜¤ë¥˜: {str(e)}"
        state["combined_results"] = []
        state["quality_score"] = 0.0

    return state

def enrich_with_summaries_node(state: BidSearchState) -> BidSearchState:
    """Supabaseì—ì„œ GPT ìš”ì•½ ì¶”ê°€ ë…¸ë“œ"""
    try:
        supabase = init_supabase_client()
        if not supabase:
            return state
        
        # ê° ê³µê³ ì— ëŒ€í•´ ìš”ì•½ ì •ë³´ ì¶”ê°€
        for result in state["combined_results"]:
            bid_no = result.get("bidNtceNo")
            if bid_no:
                # ìš”ì•½ ì •ë³´ ì¡°íšŒ
                summary, created_at, summary_type = get_bid_summary(bid_no)
                result["summary"] = summary if summary != "ì´ ê³µê³ ì— ëŒ€í•œ ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." else ""
                result["summary_type"] = summary_type or "none"
                result["summary_created_at"] = created_at
        
        state["status_messages"] = ["âœ… GPT ìš”ì•½ ì •ë³´ ì¶”ê°€ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"ìš”ì•½ ì •ë³´ ì¶”ê°€ ì˜¤ë¥˜: {str(e)}"
        
    return state

def generate_answer_node(state: BidSearchState) -> BidSearchState:
    """AI ë‹µë³€ ìƒì„± ë…¸ë“œ (RAG ì ìš©) - Supabase ë²„ì „"""
    try:
        llm = init_langchain_llm()
        if not llm:
            state["final_answer"] = "AI ë‹µë³€ ìƒì„±ì„ ìœ„í•œ OpenAI ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤."
            return state

        if not state["combined_results"]:
            state["final_answer"] = f"'{state['query']}'ì— ëŒ€í•œ ì…ì°° ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state
        
        # RAG (Retrieval-Augmented Generation) ë‹µë³€ ìƒì„±
        contexts = []
        for i, result in enumerate(state["combined_results"][:5]):
            bid_no = result.get("bidNtceNo", "N/A")
            title = result.get("bidNtceNm", "ì œëª© ì—†ìŒ")
            org = result.get("ntceInsttNm", "ê¸°ê´€ëª… ì—†ìŒ")
            summary = result.get("summary", "ìš”ì•½ ì—†ìŒ")
            amount = convert_to_won_format(result.get("asignBdgtAmt", 0))
            source = result.get("source", "unknown")

            context_item = f"""
ê³µê³  {i+1} (ì¶œì²˜: {source}):
- ê³µê³ ë²ˆí˜¸: {bid_no}
- ê³µê³ ëª…: {title}
- ê¸°ê´€: {org}
- ê¸ˆì•¡: {amount}
- ìš”ì•½: {summary}
"""
            contexts.append(context_item)

        context_text = "\n".join(contexts)

        # LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ RAG ì ìš©
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì…ì°° ê³µê³  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."),
            ("human", f"ë‹¤ìŒì€ ì…ì°° ê³µê³  ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{context_text}\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {state['query']}")
        ])

        # LangChain ì²´ì¸ ì‹¤í–‰
        chain = prompt | llm | StrOutputParser()
        answer = chain.invoke({
            "query": state["query"],
            "context": context_text
        })

        state["final_answer"] = answer
        state["status_messages"] = ["âœ… AI ë¶„ì„ ë‹µë³€ ìƒì„± ì™„ë£Œ!"]

    except Exception as e:
        state["error"] = f"AI ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
        state["final_answer"] = "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    return state

def check_error(state: BidSearchState) -> str:
    """ì—ëŸ¬ ì²´í¬ ë…¸ë“œ"""
    if state.get("error"):
        return "error"
    return "continue"

# ========== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ==========
def create_bid_search_workflow():
    """ì…ì°° ê³µê³  ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„± - Supabase ë²„ì „"""
    workflow = StateGraph(BidSearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("preprocess_query", preprocess_query_node)
    workflow.add_node("search_supabase", search_supabase_node)  # MongoDB -> Supabase
    workflow.add_node("search_vector_db", search_vector_db_node)
    workflow.add_node("combine_results", combine_results_node)
    workflow.add_node("enrich_summaries", enrich_with_summaries_node)
    workflow.add_node("generate_answer", generate_answer_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("preprocess_query")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "preprocess_query",
        check_error,
        {
            "continue": "search_supabase",
            "error": END
        }
    )
    
    # ë³‘ë ¬ ê²€ìƒ‰ í›„ ê²°í•©
    workflow.add_edge("search_supabase", "search_vector_db")
    workflow.add_edge("search_vector_db", "combine_results")
    workflow.add_edge("combine_results", "enrich_summaries")
    workflow.add_edge("enrich_summaries", "generate_answer")
    workflow.add_edge("generate_answer", END)
    
    # ì²´í¬í¬ì¸í„° ì¶”ê°€
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

# ========== LangGraph í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œë“¤ (Supabase ë²„ì „) ==========
def search_supabase_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """Supabaseì—ì„œ ê²€ìƒ‰ - í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „"""
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"Supabase ê²€ìƒ‰ ì‹œì‘: {query}")
    )
    
    try:
        # Supabase í…ìŠ¤íŠ¸ ê²€ìƒ‰
        results = search_bids_by_text(query, limit=50)
        
        # ìµœê·¼ 30ì¼ ë°ì´í„° í•„í„°ë§
        from datetime import datetime, timedelta
        date_30_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
        
        filtered_results = []
        for result in results:
            bid_date = result.get('bidNtceDate', '')
            if bid_date and bid_date >= date_30_days_ago:
                filtered_results.append(result)
        
        state["supabase_results"] = filtered_results
        state["search_method"].append("Supabase")
        state["messages"].append(
            AIMessage(content=f"Supabaseì—ì„œ {len(filtered_results)}ê±´ ê²€ìƒ‰ë¨")
        )
        
    except Exception as e:
        state["messages"].append(
            AIMessage(content=f"Supabase ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["supabase_results"] = []
    
    return state

def search_vector_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """VectorDBì—ì„œ ì‹œë§¨í‹± ê²€ìƒ‰ - í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „"""
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"VectorDB ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œì‘")
    )
    
    try:
        vector_store = init_vector_store()
        if not vector_store:
            state["messages"].append(
                AIMessage(content="ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨")
            )
            state["vector_results"] = []
            return state
        
        # ë²¡í„° ê²€ìƒ‰
        results = vector_store.similarity_search_with_score(query, k=30)
        
        vector_results = []
        for doc, score in results:
            metadata = doc.metadata
            similarity = 1 / (1 + score)
            
            if similarity >= 0.03:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                # Supabaseì—ì„œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
                bid_no = metadata.get('ê³µê³ ë²ˆí˜¸', '')
                if bid_no:
                    bid_detail = get_bid_detail_by_no(bid_no)
                    if bid_detail:
                        vector_results.append({
                            "content": doc.page_content[:500],
                            "metadata": {**metadata, **bid_detail},
                            "similarity": similarity,
                            "bidNtceNo": bid_no,
                            "bidNtceNm": bid_detail.get('bidNtceNm', metadata.get('ê³µê³ ëª…', 'N/A')),
                            "ntceInsttNm": bid_detail.get('ntceInsttNm', metadata.get('ê¸°ê´€ëª…', 'N/A'))
                        })
        
        state["vector_results"] = vector_results
        state["search_method"].append("VectorDB")
        state["messages"].append(
            AIMessage(content=f"VectorDBì—ì„œ {len(vector_results)}ê±´ ê²€ìƒ‰ë¨ (ìœ ì‚¬ë„ 0.03 ì´ìƒ)")
        )
        
    except Exception as e:
        state["messages"].append(
            AIMessage(content=f"VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["vector_results"] = []
    
    return state

def check_need_api_node(state: HybridSearchState) -> HybridSearchState:
    """API ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨"""
    supabase_count = len(state["supabase_results"])
    vector_count = len(state["vector_results"])
    total_count = supabase_count + vector_count
    
    state["messages"].append(
        HumanMessage(content=f"ê²€ìƒ‰ ê²°ê³¼ í™•ì¸: Supabase {supabase_count}ê±´, VectorDB {vector_count}ê±´")
    )
    
    # ê²°ê³¼ê°€ 10ê°œ ë¯¸ë§Œì´ë©´ API ê²€ìƒ‰ í•„ìš”
    if total_count < 10:
        state["need_api_search"] = True
        state["messages"].append(
            AIMessage(content=f"ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({total_count}ê±´), API ê²€ìƒ‰ í•„ìš”")
        )
    else:
        state["need_api_search"] = False
        state["messages"].append(
            AIMessage(content=f"ì¶©ë¶„í•œ ê²€ìƒ‰ ê²°ê³¼ ({total_count}ê±´), API ê²€ìƒ‰ ë¶ˆí•„ìš”")
        )
    
    return state

def fetch_naratang_api_node(state: HybridSearchState) -> HybridSearchState:
    """ë‚˜ë¼ì¥í„° API í˜¸ì¶œ (í•„ìš”í•œ ê²½ìš°ë§Œ)"""
    if not state["need_api_search"]:
        state["api_results"] = {}
        return state
    
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"ë‚˜ë¼ì¥í„° API ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œì‘")
    )
    
    # ë‚ ì§œ ì„¤ì •
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    all_results = {}
    api_total = 0
    
    for category, endpoint in API_ENDPOINTS.items():
        try:
            state["messages"].append(
                AIMessage(content=f"{category} ì¹´í…Œê³ ë¦¬ API í˜¸ì¶œ ì¤‘...")
            )
            
            params = {
                'serviceKey': NARATANG_API_KEY,
                'pageNo': 1,
                'numOfRows': 20,
                'inqryDiv': 1,
                'type': 'json',
                'inqryBgnDt': start_date.strftime('%Y%m%d') + '0000',
                'inqryEndDt': end_date.strftime('%Y%m%d') + '2359',
                'bidNtceNm': query
            }
            
            query_string = urlencode(params, quote_via=quote_plus)
            url = f"{endpoint}?{query_string}"
            
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('response', {}).get('body', {}).get('items', [])
                
                if items:
                    all_results[category] = items
                    api_total += len(items)
                    state["messages"].append(
                        AIMessage(content=f"{category}: {len(items)}ê±´ ê²€ìƒ‰ ì™„ë£Œ")
                    )
                
        except Exception as e:
            state["messages"].append(
                AIMessage(content=f"{category} API ì˜¤ë¥˜: {str(e)}")
            )
    
    state["api_results"] = all_results
    state["search_method"].append("API")
    state["messages"].append(
        AIMessage(content=f"API ê²€ìƒ‰ ì™„ë£Œ: ì´ {api_total}ê±´")
    )
    
    return state

def combine_hybrid_results_node(state: HybridSearchState) -> HybridSearchState:
    """ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±° - Supabase ë²„ì „"""
    state["messages"].append(
        HumanMessage(content="ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì¤‘...")
    )
    
    combined_dict = {}
    
    # Supabase ê²°ê³¼ ì¶”ê°€
    for item in state["supabase_results"]:
        bid_no = item.get("bidNtceNo")
        if bid_no:
            combined_dict[bid_no] = {
                **item,
                "source": "Supabase",
                "relevance_score": 5  # ê¸°ë³¸ ì ìˆ˜
            }
    
    # VectorDB ê²°ê³¼ ì¶”ê°€/ë³‘í•©
    for item in state["vector_results"]:
        bid_no = item.get("bidNtceNo")
        if bid_no:
            if bid_no in combined_dict:
                # ì´ë¯¸ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
                combined_dict[bid_no]["relevance_score"] += item["similarity"] * 10
                combined_dict[bid_no]["vector_content"] = item.get("content", "")
                combined_dict[bid_no]["source"] += ", VectorDB"
            else:
                # Supabaseì—ì„œ ì¶”ê°€ ì •ë³´ ì¡°íšŒ
                bid_detail = get_bid_detail_by_no(bid_no)
                
                if bid_detail:
                    combined_dict[bid_no] = {
                        **bid_detail,
                        "source": "VectorDB",
                        "relevance_score": item["similarity"] * 10,
                        "vector_content": item.get("content", "")
                    }
    
    # API ê²°ê³¼ ì¶”ê°€
    for category, items in state["api_results"].items():
        for item in items:
            bid_no = item.get("bidNtceNo")
            if bid_no:
                if bid_no in combined_dict:
                    combined_dict[bid_no]["relevance_score"] += 3
                    combined_dict[bid_no]["source"] += ", API"
                else:
                    combined_dict[bid_no] = {
                        **item,
                        "source": f"API({category})",
                        "relevance_score": 3,
                        "category": category
                    }
    
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ë° ì •ë ¬
    combined_results = list(combined_dict.values())
    combined_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    state["combined_results"] = combined_results
    state["total_count"] = len(combined_results)
    
    state["messages"].append(
        AIMessage(content=f"í†µí•© ì™„ë£Œ: ì´ {len(combined_results)}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
    )
    
    return state

def generate_hybrid_summary_node(state: HybridSearchState) -> HybridSearchState:
    """AI ìš”ì•½ ìƒì„± - í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „"""
    if not state["combined_results"]:
        state["summary"] = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        state["final_results"] = []
        return state
    
    state["messages"].append(
        HumanMessage(content="AI ìš”ì•½ ìƒì„± ì¤‘...")
    )
    
    # ìƒìœ„ 20ê°œë§Œ ìµœì¢… ê²°ê³¼ë¡œ
    state["final_results"] = state["combined_results"][:20]
    
    # ì†ŒìŠ¤ë³„ í†µê³„
    source_stats = {}
    for item in state["final_results"]:
        sources = item.get("source", "").split(", ")
        for source in sources:
            source_stats[source] = source_stats.get(source, 0) + 1
    
    # ìš”ì•½ ìƒì„±
    summary_parts = [
        f"ğŸ” '{state['query']}' ê²€ìƒ‰ ê²°ê³¼: ì´ {state['total_count']}ê±´",
        f"\nğŸ“Š ê²€ìƒ‰ ì†ŒìŠ¤:"
    ]
    
    for source, count in source_stats.items():
        summary_parts.append(f"  â€¢ {source}: {count}ê±´")
    
    # ìƒìœ„ 3ê°œ ê³µê³  í•˜ì´ë¼ì´íŠ¸
    summary_parts.append(f"\nğŸ† ìƒìœ„ ê³µê³ :")
    for i, item in enumerate(state["final_results"][:3], 1):
        title = item.get('bidNtceNm', 'ì œëª© ì—†ìŒ')
        org = item.get('ntceInsttNm', 'N/A')
        amount = item.get('asignBdgtAmt', 0)
        summary_parts.append(
            f"{i}. {title[:40]}..."
            f" ({org}, {convert_to_won_format(amount)})"
        )
    
    state["summary"] = "\n".join(summary_parts)
    
    state["messages"].append(
        AIMessage(content="ìš”ì•½ ìƒì„± ì™„ë£Œ")
    )
    
    return state

def should_search_api(state: HybridSearchState) -> str:
    """API ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    if state["need_api_search"]:
        return "search_api"
    else:
        return "combine"

# ========== í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ìƒì„± ==========
def create_hybrid_search_workflow():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„± - Supabase ë²„ì „"""
    workflow = StateGraph(HybridSearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_supabase", search_supabase_hybrid_node)
    workflow.add_node("search_vector", search_vector_hybrid_node)
    workflow.add_node("check_need_api", check_need_api_node)
    workflow.add_node("search_api", fetch_naratang_api_node)
    workflow.add_node("combine", combine_hybrid_results_node)
    workflow.add_node("generate_summary", generate_hybrid_summary_node)
    
    # ì—£ì§€ ì„¤ì •
    workflow.set_entry_point("search_supabase")
    workflow.add_edge("search_supabase", "search_vector")
    workflow.add_edge("search_vector", "check_need_api")
    
    # ì¡°ê±´ë¶€ ì—£ì§€: API ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    workflow.add_conditional_edges(
        "check_need_api",
        should_search_api,
        {
            "search_api": "search_api",
            "combine": "combine"
        }
    )
    
    workflow.add_edge("search_api", "combine")
    workflow.add_edge("combine", "generate_summary")
    workflow.add_edge("generate_summary", END)
    
    return workflow.compile()

# ========== UI íƒ­ í•¨ìˆ˜ë“¤ ==========

# tab1: ì‹¤ì‹œê°„ ì…ì°° ê³µê³ 
def show_live_bids_tab(df_live):
    """ì‹¤ì‹œê°„ ì…ì°° ê³µê³  íƒ­ UI êµ¬ì„± - Supabase ë²„ì „"""
    st.markdown("""
    <style>
        .main-header-2 {
            text-align: center;
            padding: 3.5rem 0;
            background: linear-gradient(135deg, #ff9a8b 0%, #ff6a88 100%);
            color: white;
            border-radius: 20px;
            margin-bottom: 3rem;
            box-shadow: 0 10px 20px rgba(0,0,0,0.25);
            overflow: hidden;
            position: relative;
            animation: fadeIn 1.5s ease-out;
        }
        .main-header-2::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
            animation: rotateBg 20s linear infinite;
            opacity: 0.7;
        }
        .main-header-2 h1 {
            font-size: 3.8rem;
            font-weight: 900;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            position: relative;
            z-index: 1;
            margin-bottom: 0.8rem;
        }
        .main-header-2 p {
            font-size: 1.5rem;
            font-weight: 400;
            opacity: 0.95;
            line-height: 1.6;
            max-width: 900px;
            margin: 0.8rem auto 0 auto;
            position: relative;
            z-index: 1;
        }
        @keyframes rotateBg {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="main-header-2">
        <h1>ğŸš€ ë‹¹ì‹ ì˜ ì…ì°° ì„±ê³µ íŒŒíŠ¸ë„ˆ, AI ì…ì°° ë„ìš°ë¯¸!</h1>
        <p>
            ë§¤ì¼ ì—…ë°ì´íŠ¸ë˜ëŠ” ì‹¤ì‹œê°„ ê³µê³  í™•ì¸ë¶€í„°<br>
            ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ì˜ ì •í™•í•œ ê²€ìƒ‰ê³¼ ìŠ¤ë§ˆíŠ¸í•œ ì§ˆì˜ì‘ë‹µê¹Œì§€,<br>
            ë³µì¡í•œ ì…ì°° ê³¼ì •ì„ ì‰½ê³  ë¹ ë¥´ê²Œ ê²½í—˜í•˜ì„¸ìš”.
        </p>
    </div>
    """, unsafe_allow_html=True) 
         
    st.subheader("ğŸ“¢ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì…ì°° ëª©ë¡")
    
    # DataFrameì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if df_live.empty:
        st.warning("í˜„ì¬ í‘œì‹œí•  ì…ì°° ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì¤€ë¹„ - ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
    df_live_display = df_live.copy()
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (JSONBì—ì„œ ì¶”ì¶œëœ ë°ì´í„°)
    display_columns = {
        "bidNtceNo": "ê³µê³ ë²ˆí˜¸",
        "bidNtceNm": "ê³µê³ ëª…", 
        "ntceInsttNm": "ê³µê³ ê¸°ê´€",
        "bsnsDivNm": "ë¶„ë¥˜",
        "asignBdgtAmt": "ê¸ˆì•¡",
        "bidNtceDate": "ê²Œì‹œì¼",
        "bidClseDate": "ë§ˆê°ì¼",
        "bidClseTm": "ë§ˆê°ì‹œê°„",
        "bidNtceUrl": "url",
        "bidNtceBgn": "ê²Œì‹œì‹œê°„",
        "bidNtceSttusNm": "ì…ì°°ê³µê³ ìƒíƒœëª…",
        "dmndInsttNm": "ìˆ˜ìš”ê¸°ê´€",
        "bidprcPsblIndstrytyNm": "íˆ¬ì°°ê°€ëŠ¥ì—…ì¢…ëª…",
        "cmmnReciptMethdNm": "ê³µë™ìˆ˜ê¸‰",
        "rgnLmtYn": "ì§€ì—­ì œí•œ",
        "prtcptPsblRgnNm": "ì°¸ê°€ê°€ëŠ¥ì§€ì—­ëª…",
        "presmptPrce": "ì¶”ì •ê°€ê²©"
    }
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    available_columns = [col for col in display_columns.keys() if col in df_live_display.columns]
    df_live_display = df_live_display[available_columns]
    df_live_display.columns = [display_columns[col] for col in available_columns]

    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    if "ë§ˆê°ì¼" in df_live_display.columns:
        df_live_display["ë§ˆê°ì¼"] = pd.to_datetime(df_live_display["ë§ˆê°ì¼"], errors='coerce')
    if "ê²Œì‹œì¼" in df_live_display.columns:
        df_live_display["ê²Œì‹œì¼"] = pd.to_datetime(df_live_display["ê²Œì‹œì¼"], errors='coerce')
    
    df_live_display = df_live_display.sort_values(by=['ê²Œì‹œì¼','ê²Œì‹œì‹œê°„'], ascending=False)

    # í•„í„° UI
    search_keyword = st.text_input("ğŸ” ê³µê³ ëª… ë˜ëŠ” ê³µê³ ê¸°ê´€ ê²€ìƒ‰")
    unique_categories = ["ê³µì‚¬", "ìš©ì—­", "ë¬¼í’ˆ", "ì™¸ì"]
    selected_cls = st.multiselect("ğŸ“ ë¶„ë¥˜ ì„ íƒ", 
                                 options=unique_categories, 
                                 default=unique_categories)

    col2, col3, col4 = st.columns(3)        
    with col2:
        if not df_live_display["ê²Œì‹œì¼"].empty:
            start_date = st.date_input("ğŸ“… ê²Œì‹œì¼ ê¸°ì¤€ ì‹œì‘ì¼", value=df_live_display["ê²Œì‹œì¼"].min().date())
        else:
            start_date = st.date_input("ğŸ“… ê²Œì‹œì¼ ê¸°ì¤€ ì‹œì‘ì¼", value=datetime.now().date())
    with col3:
        if not df_live_display["ê²Œì‹œì¼"].empty:
            end_date = st.date_input("ğŸ“… ê²Œì‹œì¼ ê¸°ì¤€ ì¢…ë£Œì¼", value=df_live_display["ê²Œì‹œì¼"].max().date())
        else:
            end_date = st.date_input("ğŸ“… ê²Œì‹œì¼ ê¸°ì¤€ ì¢…ë£Œì¼", value=datetime.now().date())
    with col4:
        sort_col = st.selectbox("ì •ë ¬ê¸°ì¤€",options=["ì‹¤ì‹œê°„","ê²Œì‹œì¼","ë§ˆê°ì¼","ê¸ˆì•¡"])
        if sort_col == "ì‹¤ì‹œê°„" :
            sort_order = "ë‚´ë¦¼ì°¨ìˆœ"
            st.empty()
        else :
            sort_order = st.radio("ì •ë ¬ ë°©í–¥", options=["ì˜¤ë¦„ì°¨ìˆœ", "ë‚´ë¦¼ì°¨ìˆœ"], horizontal=True,
                                label_visibility="collapsed")
   
    # í•„í„°ë§ ì ìš©
    filtered = df_live_display.copy()

    if selected_cls and "ë¶„ë¥˜" in filtered.columns:
        filtered = filtered[filtered["ë¶„ë¥˜"].isin(selected_cls)]

    if search_keyword:
        mask = pd.Series([False] * len(filtered))
        if "ê³µê³ ëª…" in filtered.columns:
            mask |= filtered["ê³µê³ ëª…"].str.contains(search_keyword, case=False, na=False, regex=False)
        if "ê³µê³ ê¸°ê´€" in filtered.columns:
            mask |= filtered["ê³µê³ ê¸°ê´€"].str.contains(search_keyword, case=False, na=False, regex=False)
        if "ê³µê³ ë²ˆí˜¸" in filtered.columns:
            mask |= filtered["ê³µê³ ë²ˆí˜¸"].str.contains(search_keyword, case=False, na=False, regex=False)
        filtered = filtered[mask]

    if "ê²Œì‹œì¼" in filtered.columns:
        filtered = filtered[
            (filtered["ê²Œì‹œì¼"].dt.date >= start_date) & 
            (filtered["ê²Œì‹œì¼"].dt.date <= end_date)
        ]

    # ì •ë ¬ ì ìš©
    ascending = True if sort_order == "ì˜¤ë¦„ì°¨ìˆœ" else False

    if sort_col == "ì‹¤ì‹œê°„" and "ê²Œì‹œì¼" in filtered.columns:
        filtered = filtered.sort_values(by=["ê²Œì‹œì¼", "ê²Œì‹œì‹œê°„"], ascending=False)
    elif sort_col == "ê²Œì‹œì¼" and "ê²Œì‹œì¼" in filtered.columns:
        filtered = filtered.sort_values(by=["ê²Œì‹œì¼", "ê²Œì‹œì‹œê°„"], ascending=ascending)
    elif sort_col == "ë§ˆê°ì¼" and "ë§ˆê°ì¼" in filtered.columns:
        filtered = filtered.sort_values(by="ë§ˆê°ì¼", ascending=ascending)
    elif sort_col == "ê¸ˆì•¡" and "ê¸ˆì•¡" in filtered.columns:
        filtered = filtered.sort_values(by="ê¸ˆì•¡", ascending=ascending)

    st.markdown(f"<div style='text-align: left; margin-bottom: 10px;'>ê²€ìƒ‰ ê²°ê³¼ {len(filtered)}ê±´</div>", unsafe_allow_html=True)

    # í˜ì´ì§€ë„¤ì´ì…˜
    PAGE_SIZE = 10
    def paginate_dataframe(df, page_num, page_size):
        start_index = page_num * page_size
        end_index = (page_num + 1) * page_size
        return df.iloc[start_index:end_index]

    if "current_page" not in st.session_state:
        st.session_state["current_page"] = 0

    total_pages = (len(filtered) + PAGE_SIZE - 1) // PAGE_SIZE
    paginated_df = paginate_dataframe(filtered, st.session_state["current_page"], PAGE_SIZE)
    
    st.write("")
    st.write("")
    
    # í…Œì´ë¸” í—¤ë”
    header_cols = st.columns([1,1.5, 3.5, 2.5, 1, 1.5,1.5, 1.5, 1])
    headers = ['ê³µê³ ë²ˆí˜¸',"êµ¬ë¶„",'ê³µê³ ëª…','ê³µê³ ê¸°ê´€','ë¶„ë¥˜','ê¸ˆì•¡','ê²Œì‹œì¼','ë§ˆê°ì¼','ìƒì„¸ì •ë³´']
    for col, head in zip(header_cols, headers):
        col.markdown(f"**{head}**")

    st.markdown("<hr style='margin-top: 5px; margin-bottom: 10px;'>", unsafe_allow_html=True)

    # í–‰ ë Œë”ë§
    for i, (idx, row) in enumerate(paginated_df.iterrows()):
        cols = st.columns([1,1.5, 3.5, 2.5, 1, 1.5,1.5, 1.5, 1])
        cols[0].write(row.get("ê³µê³ ë²ˆí˜¸", ""))
        cols[1].write(row.get("ì…ì°°ê³µê³ ìƒíƒœëª…", ""))
        cols[2].markdown(row.get("ê³µê³ ëª…", ""))
        cols[3].write(row.get("ê³µê³ ê¸°ê´€", ""))
        cols[4].write(row.get("ë¶„ë¥˜", ""))            
        
        # ê¸ˆì•¡ í‘œì‹œ (ì¶”ì •ê°€ê²©ì´ ìˆìœ¼ë©´ ì¶”ì •ê°€ê²©, ì—†ìœ¼ë©´ ê¸ˆì•¡)
        if row.get("ë¶„ë¥˜") == "ê³µì‚¬" and row.get("ì¶”ì •ê°€ê²©"):
            ê¸ˆì•¡ = row.get("ì¶”ì •ê°€ê²©")
        else:
            ê¸ˆì•¡ = row.get("ê¸ˆì•¡")
        cols[5].write(convert_to_won_format(ê¸ˆì•¡))
        
        if pd.notna(row.get("ê²Œì‹œì¼")):
            cols[6].write(row["ê²Œì‹œì¼"].strftime("%Y-%m-%d"))
        else:
            cols[6].write("")
            
        if pd.notna(row.get("ë§ˆê°ì¼")):
            cols[7].write(row["ë§ˆê°ì¼"].strftime("%Y-%m-%d"))
        else:
            cols[7].write("ê³µê³  ì°¸ì¡°")
            
        if cols[8].button("ë³´ê¸°", key=f"live_detail_{i}_{idx}"):
            st.session_state["page"] = "detail"
            st.session_state["selected_live_bid"] = row.to_dict()
            st.rerun()

        st.markdown("<hr style='margin-top: 5px; margin-bottom: 10px;'>", unsafe_allow_html=True)

    # í˜ì´ì§€ ì´ë™ ë²„íŠ¼
    cols_pagination = st.columns([1, 3, 1])
    with cols_pagination[0]:
        if st.session_state["current_page"] > 0:
            if st.button("ì´ì „"):
                st.session_state["current_page"] -= 1
                st.rerun()

    with cols_pagination[2]:
        if st.session_state['current_page'] < total_pages -1:
            if st.button("ë‹¤ìŒ"):
                st.session_state["current_page"] += 1
                st.rerun()

    st.markdown(f"<div style='text-align: center;'> {st.session_state['current_page'] + 1} / {total_pages}</div>", unsafe_allow_html=True)

# tab2: ì‹œë§¨í‹± ê²€ìƒ‰
def show_semantic_search_tab():
    """ì‹œë§¨í‹± ê²€ìƒ‰ íƒ­ UI êµ¬ì„± - Supabase ë²„ì „"""
    st.markdown("""
    <style>
        .main-header-1 {
            text-align: center;
            padding: 2rem 0;
            background: linear-gradient(90deg, #4facfe 0%, #00f2fe 100%);
            color: white;
            border-radius: 20px;
            margin-bottom: 2.5rem;
            box-shadow: 0 8px 16px rgba(0,0,0,0.2);
            overflow: hidden;
            position: relative;
            animation: fadeIn 1.5s ease-out;
        }        
        .main-header-1 h1 {
            font-size: 3.5rem;
            font-weight: 800;
            letter-spacing: -0.05em;
            margin-bottom: 0.5rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        .main-header-1 p {
            font-size: 1.4rem;
            opacity: 0.9;
            line-height: 1.6;
            max-width: 800px;
            margin: 0.8rem auto 0 auto;
            position: relative;
            z-index: 1;
        }      
        .main-header-1::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, rgba(255,255,255,0.1) 0%, transparent 20%, transparent 80%, rgba(255,255,255,0.1) 100%);
            animation: scanLine 4s infinite linear;
            z-index: 0;
        }
        @keyframes scanLine {
            0% { transform: translateX(-100%); }
            50% { transform: translateX(100%); }
            100% { transform: translateX(-100%); }
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="main-header-1">
        <h1>ğŸ” ìŠ¤ë§ˆíŠ¸ AI ê²€ìƒ‰ ì—”ì§„</h1>
        <p>
            ì›í•˜ëŠ” ì…ì°° ì •ë³´ë¥¼ í‚¤ì›Œë“œ ëŒ€ì‹  ìì—°ì–´ë¡œ ê²€ìƒ‰í•˜ê³ <br>
            AIê°€ ìš”ì•½í•´ì£¼ëŠ” í•µì‹¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.
        </p>
    </div>
    """, unsafe_allow_html=True)

    # ê²€ìƒ‰ UI
    col1, col2 = st.columns([4, 1])
    with col1:
        search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", 
                                    placeholder="ì˜ˆ: ì„œë²„ êµ¬ì¶•, ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ, ê±´ì„¤ ê³µì‚¬ ë“±",
                                    key="semantic_search_input")
    with col2:
        search_button = st.button("ğŸ” ê²€ìƒ‰", key="semantic_search_btn", type="primary")
    
    # ê²€ìƒ‰ ì˜µì…˜
    with st.expander("ğŸ”§ ê²€ìƒ‰ ì˜µì…˜"):
        num_results = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", min_value=5, max_value=30, value=10, step=5)
        similarity_threshold = st.slider("ìœ ì‚¬ë„ ì„ê³„ê°’", min_value=0.0, max_value=1.0, value=0.3, step=0.01)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    if search_button and search_query:
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            # ì‹œë§¨í‹± ê²€ìƒ‰ ìˆ˜í–‰
            search_results = semantic_search_supabase(search_query, k=num_results)
            
            if search_results:
                # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
                filtered_results = [(meta, score) for meta, score in search_results if score >= similarity_threshold]
                
                st.markdown(f"### ê²€ìƒ‰ ê²°ê³¼: {len(filtered_results)}ê±´")
                
                if filtered_results:
                    # RAG ì‘ë‹µ ìƒì„±
                    rag_response = generate_rag_response_supabase(search_query, filtered_results)
                    
                    # AI ì‘ë‹µ í‘œì‹œ
                    st.markdown("#### ğŸ¤– AI ê²€ìƒ‰ ìš”ì•½")
                    st.info(rag_response)
                    
                    st.markdown("---")
                    st.markdown("#### ğŸ“‹ ê²€ìƒ‰ëœ ê³µê³  ëª©ë¡")
                    
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì‹œ
                    for idx, (metadata, score) in enumerate(filtered_results):
                        with st.container():
                            col1, col2, col3, col4 = st.columns([0.8, 3, 2, 1])
                            
                            # ìœ ì‚¬ë„ í‘œì‹œ (ìƒ‰ìƒ ì½”ë”©)
                            if score >= 0.7:
                                col1.markdown(f"<span style='color: green; font-weight: bold;'>{score:.1%}</span>", 
                                            unsafe_allow_html=True)
                            elif score >= 0.5:
                                col1.markdown(f"<span style='color: orange; font-weight: bold;'>{score:.1%}</span>", 
                                            unsafe_allow_html=True)
                            else:
                                col1.markdown(f"<span style='color: red;'>{score:.1%}</span>", 
                                            unsafe_allow_html=True)
                            
                            col2.write(f"**{metadata.get('bidNtceNm', 'N/A')}**")
                            col3.write(metadata.get('ntceInsttNm', 'N/A'))
                            
                            # ìƒì„¸ë³´ê¸° ë²„íŠ¼
                            if col4.button("ìƒì„¸", key=f"semantic_detail_{idx}"):
                                st.session_state["page"] = "detail"
                                st.session_state["selected_live_bid"] = metadata
                                st.rerun()
                                
                            # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                            with st.expander(f"ë”ë³´ê¸° - {metadata.get('bidNtceNo', 'N/A')}"):
                                st.write(f"**ë§ˆê°ì¼:** {metadata.get('bidClseDate', 'N/A')}")
                                st.write(f"**ì˜ˆì‚°:** {convert_to_won_format(metadata.get('asignBdgtAmt', 0))}")
                                st.write(f"**ë¶„ë¥˜:** {metadata.get('bsnsDivNm', 'N/A')}")
                                
                                # GPT ìš”ì•½ í‘œì‹œ
                                bid_no = metadata.get('bidNtceNo')
                                if bid_no:
                                    summary, created_at, summary_type = get_bid_summary(bid_no)
                                    if summary and summary != "ì´ ê³µê³ ì— ëŒ€í•œ ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.":
                                        st.markdown("**ğŸ“ ìš”ì•½:**")
                                        st.write(summary)
                                
                                st.divider()
                else:
                    st.warning(f"ìœ ì‚¬ë„ {similarity_threshold:.1%} ì´ìƒì¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”.")
            else:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

# tab3: LangGraph AI ê²€ìƒ‰
def add_langgraph_search_tab():
    """LangGraph AI ê²€ìƒ‰ íƒ­ UI - Supabase ë²„ì „"""   
    st.markdown("""
    <style>
        .langgraph-header-option2 {
            text-align: center;
            padding: 2rem 0;
            background: linear-gradient(135deg, #3a1c71 0%, #d76d77 50%, #ffaf7b 100%);
            color: white;
            border-radius: 15px;
            margin-bottom: 2.5rem;
            box-shadow: 0 8px 20px rgba(0,0,0,0.35);
            overflow: hidden;
            position: relative;
            animation: fadeIn 1.5s ease-out;
        }
        .langgraph-header-option2 h1 {
            font-size: 3.5rem;
            font-weight: 900;
            text-shadow: 2px 2px 5px rgba(0,0,0,0.5);
            position: relative;
            z-index: 1;
            margin-bottom: 0.8rem;
        }
        .langgraph-header-option2 p {
            font-size: 1.4rem;
            font-weight: 400;
            opacity: 0.95;
            line-height: 1.6;
            max-width: 900px;
            margin: 0.8rem auto 0 auto;
            position: relative;
            z-index: 1;
        }
        .langgraph-header-option2::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 300%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3) 50%, transparent);
            transform: translateX(-100%);
            animation: lightSweep 10s infinite ease-in-out;
            z-index: 0;
        }
        @keyframes lightSweep {
            0% { transform: translateX(-100%); }
            50% { transform: translateX(0%); }
            100% { transform: translateX(100%); }
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="langgraph-header-option2">
        <h1>âœ¨ LangGraph ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„</h1>
        <p>
            AI ì›Œí¬í”Œë¡œìš°ë¡œ ì…ì°° ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ íŒŒì•…í•˜ê³ <br>
            ì „ëµì ì¸ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ê¹Šì´ ìˆëŠ” í†µì°°ì„ ì–»ìœ¼ì„¸ìš”.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    hybrid_workflow = create_hybrid_search_workflow()
    
    # ê²€ìƒ‰ UI
    col1, col2 = st.columns([4, 1])
    
    with col1:
        search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", 
                                   placeholder="ì˜ˆ: AI ê°œë°œ, ì„œë²„ êµ¬ì¶•, ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë“±",
                                   key="langgraph_hybrid_search_input")
    with col2:
        search_button = st.button("ğŸ” ê²€ìƒ‰", key="langgraph_hybrid_search_btn", type="primary")
    
    # ê²€ìƒ‰ ì˜µì…˜
    with st.expander("ğŸ”§ ê²€ìƒ‰ ì˜µì…˜"):
        col1, col2 = st.columns(2)
        with col1:
            min_results_for_api = st.slider("API ê²€ìƒ‰ ê¸°ì¤€ (ìµœì†Œ ê²°ê³¼ ìˆ˜)", 
                                           min_value=5, max_value=20, value=10,
                                           help="DB ê²€ìƒ‰ ê²°ê³¼ê°€ ì´ ìˆ˜ì¹˜ ë¯¸ë§Œì¼ ë•Œ API ê²€ìƒ‰ ì‹¤í–‰")
        with col2:
            show_process = st.checkbox("ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ í‘œì‹œ", value=True)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    if search_button and search_query:
        # í”„ë¡œì„¸ìŠ¤ ì»¨í…Œì´ë„ˆ
        if show_process:
            process_container = st.container()
        
        with st.spinner("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘..."):
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "query": search_query,
                "search_method": [],
                "supabase_results": [],
                "vector_results": [],
                "api_results": {},
                "combined_results": [],
                "final_results": [],
                "summary": "",
                "messages": [],
                "error": None,
                "total_count": 0,
                "need_api_search": False
            }
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            try:
                final_state = hybrid_workflow.invoke(initial_state)
                
                # í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ í‘œì‹œ
                if show_process:
                    with process_container:
                        st.markdown("### ğŸ”„ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤")
                        for msg in final_state["messages"]:
                            if isinstance(msg, HumanMessage):
                                st.markdown(f'<div style="color: #0066cc;">ğŸ‘¤ {msg.content}</div>', 
                                          unsafe_allow_html=True)
                            else:
                                st.markdown(f'<div style="color: #009900;">ğŸ¤– {msg.content}</div>', 
                                          unsafe_allow_html=True)
                
                # ê²°ê³¼ í‘œì‹œ
                if final_state["final_results"]:
                    st.markdown("---")
                    
                    # ìš”ì•½ í‘œì‹œ
                    st.success(final_state["summary"])
                    
                    # ê²€ìƒ‰ ì†ŒìŠ¤ íƒœê·¸
                    st.markdown("### ğŸ·ï¸ ì‚¬ìš©ëœ ê²€ìƒ‰ ë°©ë²•")
                    cols = st.columns(len(final_state["search_method"]))
                    for idx, method in enumerate(final_state["search_method"]):
                        with cols[idx]:
                            if method == "Supabase":
                                st.info(f"ğŸ“š {method}")
                            elif method == "VectorDB":
                                st.warning(f"ğŸ” {method}")
                            elif method == "API":
                                st.success(f"ğŸŒ {method}")
                    
                    # ìƒì„¸ ê²°ê³¼
                    st.markdown("### ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼")

                    for idx, item in enumerate(final_state["final_results"], 1):
                        with st.container():
                            col1, col2, col3, col4 = st.columns([0.5, 3, 1.5, 1])

                            with col1:
                                st.markdown(f"**{idx}**")

                            with col2:
                                st.markdown(f"**{item.get('bidNtceNm', 'ì œëª© ì—†ìŒ')}**")
                                st.caption(f"{item.get('ntceInsttNm', 'ê¸°ê´€ëª… ì—†ìŒ')} | "
                                        f"ê³µê³ ë²ˆí˜¸: {item.get('bidNtceNo', 'N/A')}")

                            with col3:
                                sources = item.get('source', 'Unknown').split(', ')
                                source_tags = []
                                for source in sources:
                                    if 'Supabase' in source:
                                        source_tags.append("ğŸ“š")
                                    elif 'VectorDB' in source:
                                        source_tags.append("ğŸ”")
                                    elif 'API' in source:
                                        source_tags.append("ğŸŒ")
                                st.write(" ".join(source_tags) + f" (ì ìˆ˜: {item.get('relevance_score', 0):.1f})")

                            with col4:
                                st.write(convert_to_won_format(item.get('asignBdgtAmt', 0)))
                                if st.button("ìƒì„¸", key=f"detail_hybrid_{idx}"):
                                    st.session_state["page"] = "detail"
                                    st.session_state["selected_live_bid"] = item
                                    st.rerun()

                            # ìƒì„¸ ì •ë³´ í‘œì‹œ
                            with st.expander("ë”ë³´ê¸°"):
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.write(f"**ë§ˆê°ì¼:** {item.get('bidClseDate', 'N/A')}")
                                    st.write(f"**ë¶„ë¥˜:** {item.get('bsnsDivNm', 'N/A')}")
                                    st.write(f"**ê²€ìƒ‰ ì†ŒìŠ¤:** {item.get('source', 'N/A')}")
                                with col2:
                                    st.write(f"**ì…ì°°ë°©ë²•:** {item.get('bidMthdNm', 'N/A')}")
                                    st.write(f"**ì§€ì—­ì œí•œ:** {item.get('rgnLmtYn', 'N/A')}")
                                    st.write(f"**ê´€ë ¨ë„ ì ìˆ˜:** {item.get('relevance_score', 0):.1f}")

                                # ë²¡í„° ì½˜í…ì¸ ê°€ ìˆìœ¼ë©´ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
                                if 'vector_content' in item:
                                    st.markdown("**ğŸ“„ ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:**")
                                    st.text(item['vector_content'][:300] + "...")
                                
                                # AI ìš”ì•½ì´ ìˆì„ ê²½ìš° ì¶”ê°€
                                bid_no = item.get('bidNtceNo')
                                if bid_no:
                                    summary, created_at, summary_type = get_bid_summary(bid_no)
                                    if summary and summary != "ì´ ê³µê³ ì— ëŒ€í•œ ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.":
                                        created_info = f"({summary_type} - ìƒì„±ì¼: {created_at})" if created_at else ""
                                        st.markdown(f"**ğŸ“ AI ìš”ì•½:** {summary} {created_info}")

                            st.markdown("---")
                else:
                    st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # LangGraph ì •ë³´
    with st.expander("ğŸš€ LangGraph í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŠ¹ì§•", expanded=False):
        st.markdown("""
        **ì§€ëŠ¥í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ (Supabase ë²„ì „):**
        
        **ğŸ”„ 3ë‹¨ê³„ ìˆœì°¨ ê²€ìƒ‰:**
        1. **Supabase ê²€ìƒ‰** - JSONB ê¸°ë°˜ ì •í™•í•œ ë§¤ì¹­
        2. **VectorDB ê²€ìƒ‰** - AI ì‹œë§¨í‹± ìœ ì‚¬ë„ ê²€ìƒ‰  
        3. **ë‚˜ë¼ì¥í„° API** - ì‹¤ì‹œê°„ ìµœì‹  ë°ì´í„° (í•„ìš”ì‹œë§Œ)
        
        **ğŸ¯ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì „ëµ:**
        - DB ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ API í˜¸ì¶œ ìƒëµ (ì„±ëŠ¥ ìµœì í™”)
        - ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œë§Œ ì‹¤ì‹œê°„ API ê²€ìƒ‰ ì‹¤í–‰
        - ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ê¸°ë°˜ ì •ë ¬
        
        **ğŸ“Š í†µí•© ì ìˆ˜ ì‹œìŠ¤í…œ:**
        - ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°œê²¬ëœ ê³µê³ ëŠ” ë†’ì€ ì ìˆ˜
        - Supabase ê¸°ë³¸ ì ìˆ˜: 5ì 
        - VectorDB ìœ ì‚¬ë„ ì ìˆ˜: 0~10ì 
        - API ì¶”ê°€ ì ìˆ˜: 3ì 
        
        **âœ¨ Supabase íŠ¹í™” ì¥ì :**
        - JSONB ê¸°ë°˜ ìœ ì—°í•œ ë°ì´í„° êµ¬ì¡°
        - PostgreSQLì˜ ê°•ë ¥í•œ ê²€ìƒ‰ ê¸°ëŠ¥
        - pgvectorë¥¼ í†µí•œ ë²¡í„° ê²€ìƒ‰
        - ì‹¤ì‹œê°„ í™•ì¥ì„±ê³¼ ê´€ë¦¬ í¸ì˜ì„±
        """)

# tab4: AI ë„ìš°ë¯¸
def add_chatbot_to_streamlit():
    """Streamlit ì•±ì— ì±—ë´‡ ê¸°ëŠ¥ ì¶”ê°€ - Supabase ë²„ì „"""
    st.markdown("""
    <style>
        .chatbot-header-4 {
            text-align: center;
            padding: 2rem 0;
            background: linear-gradient(90deg, #ff7e5f 0%, #feb47b 100%);
            color: white;
            border-radius: 20px;
            margin-bottom: 1.5rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.08);
            overflow: hidden;
            position: relative;
            animation: fadeIn 1.5s ease-out;
        }
        .chatbot-header-4 h1 {
            font-size: 3.5rem;
            font-weight: 800;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        .chatbot-header-4 p {
            font-size: 1.4rem;
            opacity: 0.8;
            line-height: 1.6;
            max-width: 900px;
            margin: 0.8rem auto 0 auto;
            position: relative;    
            z-index: 1;
        }  
        .chatbot-header-4::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: url('data:image/svg+xml;utf8,<svg width="100%" height="100%" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="7" fill="rgba(255,255,255,0.2)"/><circle cx="30" cy="50" r="10" fill="rgba(255,255,255,0.18)"/><circle cx="70" cy="20" r="8" fill="rgba(255,255,255,0.17)"/><circle cx="90" cy="80" r="9" fill="rgba(255,255,255,0.19)"/><circle cx="50" cy="90" r="6" fill="rgba(255,255,255,0.16)"/></svg>') repeat;
            background-size: 20% 20%;
            animation: bubbleFloat 20s infinite linear;
            z-index: 0;
        }
        @keyframes bubbleFloat {
            0% { transform: translateY(0) translateX(0); opacity: 0.8; }
            25% { transform: translateY(-5%) translateX(5%); opacity: 0.9; }
            50% { transform: translateY(-10%) translateX(0); opacity: 0.8; }
            75% { transform: translateY(-5%) translateX(-5%); opacity: 0.9; }
            100% { transform: translateY(0) translateX(0); opacity: 0.8; }
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="chatbot-header-4">
        <h1>ğŸ¤– AI ì±—ë´‡ ë„ìš°ë¯¸</h1>
        <p>
            ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”.<br>
            AIê°€ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ë“œë¦´ê²Œìš”!
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì˜ˆì‹œ ì§ˆë¬¸
    example_questions = [
        "AI ê°œë°œ ê´€ë ¨ ì…ì°° ê³µê³ ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
        "ì„œë²„ êµ¬ì¶• ì…ì°° í˜„í™©ì€ ì–´ë–¤ê°€ìš”?",
        "ìµœê·¼ ê³µê³ ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì…ì°°ì€?",
        "1ì–µì› ì´ìƒ IT ì…ì°° ê³µê³ ê°€ ìˆë‚˜ìš”?",
        "ì˜¤ëŠ˜ ë§ˆê°ë˜ëŠ” ì…ì°° ê³µê³ ëŠ”?",
        "íŠ¹ì • ê¸°ê´€ì˜ ì…ì°° ê³µê³ ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
    ]

    cols = st.columns(6)
    for idx, question in enumerate(example_questions):
        if cols[idx % 6].button(question, key=f"example_{question}"):
            st.session_state.pending_question = question
            st.rerun()
    
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_messages = []
        if 'pending_question' in st.session_state:
            del st.session_state.pending_question
        st.rerun()

    chatbot = init_chatbot()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬
    if 'pending_question' in st.session_state:
        question = st.session_state.pending_question
        del st.session_state.pending_question
        process_question(question, chatbot)
        st.rerun()
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: AI ê´€ë ¨ ì…ì°° ê³µê³ ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”)"):
        process_question(prompt, chatbot)

# ========== ìƒì„¸ í˜ì´ì§€ ==========
def show_detail_page():
    """ìƒì„¸ í˜ì´ì§€ í‘œì‹œ - Supabase ë²„ì „"""
    if st.button("â¬…ï¸ ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        st.session_state["page"] = "home"
        st.rerun()

    if "selected_live_bid" in st.session_state:
        row = st.session_state["selected_live_bid"]
        
        # ë‚ ì§œ ì²˜ë¦¬
        ë§ˆê°ì¼ = row.get('ë§ˆê°ì¼') or row.get('bidClseDate')
        ë§ˆê°ì‹œê°„ = row.get('ë§ˆê°ì‹œê°„') or row.get('bidClseTm')
        
        if isinstance(ë§ˆê°ì¼, str):
            try:
                ë§ˆê°ì¼ = pd.to_datetime(ë§ˆê°ì¼)
            except:
                ë§ˆê°ì¼ = None
        
        ë§ˆê°ì¼_í‘œì‹œ = ë§ˆê°ì¼.strftime("%Yë…„ %mì›” %dì¼") if pd.notna(ë§ˆê°ì¼) else "ê³µê³  ì°¸ì¡°"
        ë§ˆê°ì‹œê°„_í‘œì‹œ = ë§ˆê°ì‹œê°„ if pd.notna(ë§ˆê°ì‹œê°„) else "ê³µê³  ì°¸ì¡°"

        ê²Œì‹œì¼ = row.get('ê²Œì‹œì¼') or row.get('bidNtceDate')
        if isinstance(ê²Œì‹œì¼, str):
            try:
                ê²Œì‹œì¼ = pd.to_datetime(ê²Œì‹œì¼)
            except:
                ê²Œì‹œì¼ = None
        ê²Œì‹œì¼_í‘œì‹œ = ê²Œì‹œì¼.strftime("%Yë…„ %mì›” %dì¼") if pd.notna(ê²Œì‹œì¼) else "ì •ë³´ ì—†ìŒ"

        # ìƒë‹¨ ì •ë³´ ì¹´ë“œ
        st.markdown(
            f"""
            <div style="
                background-color: #e0f2f7; 
                padding: 25px 20px; 
                border-radius: 15px; 
                margin-bottom: 30px;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            ">
                <h2 style="color: #0056b3; margin-top: 0px; margin-bottom: 10px; font-weight: bold; font-size: 2.2em;">
                    {row.get('ê³µê³ ëª…') or row.get('bidNtceNm', 'ê³µê³ ëª… ì—†ìŒ')}
                </h2>
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
                    <span style="font-size: 1.2em; font-weight: 600; color: #333;">
                        ğŸ“Š êµ¬ë¶„: {row.get('ì…ì°°ê³µê³ ìƒíƒœëª…') or row.get('bidNtceSttusNm', 'ì •ë³´ ì—†ìŒ')}
                    </span>
                    <span style="font-size: 1.2em; font-weight: 600; color: #333;">
                        ğŸ¢ ìˆ˜ìš”ê¸°ê´€: {row.get('ìˆ˜ìš”ê¸°ê´€') or row.get('dmndInsttNm', 'ì •ë³´ ì—†ìŒ')}
                    </span>
                </div>
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
                    <span style="font-size: 1.2em; font-weight: 600; color: #333;">
                        ğŸ“… ê²Œì‹œì¼: {ê²Œì‹œì¼_í‘œì‹œ}
                    </span>                   
                </div>
                    <span style="font-size: 1.2em; font-weight: 600; color: #333;">
                            â³ ê³µê³ ë§ˆê°ì¼: {ë§ˆê°ì¼_í‘œì‹œ} {ë§ˆê°ì‹œê°„_í‘œì‹œ}
                    </span>
                    <div style="font-size: 1.5em; font-weight: bold; color: #007bff; text-align: right;">
                        ğŸ’° ê¸ˆì•¡: {format_won(str(row.get('ê¸ˆì•¡') or row.get('asignBdgtAmt', '0')))}
                    </div>
            </div>
            """, unsafe_allow_html=True
        )

        # ìƒì„¸ ì •ë³´ ì¹´ë“œë“¤
        col1, col2, col3 = st.columns([1,1,1])       
        
        with col1:
            ê³µë™ìˆ˜ê¸‰ = row.get('ê³µë™ìˆ˜ê¸‰') or row.get('cmmnReciptMethdNm')
            ì§€ì—­ì œí•œ = row.get('ì§€ì—­ì œí•œ') or row.get('rgnLmtYn')
            ì°¸ê°€ê°€ëŠ¥ì§€ì—­ = row.get('ì°¸ê°€ê°€ëŠ¥ì§€ì—­ëª…') or row.get('prtcptPsblRgnNm')
            
            st.markdown(
                f"""
                <div style="
                background-color: #f0fdf4;
                border: 1px solid #e5e5e5;
                padding: 25px 20px; 
                border-radius: 15px; 
                margin-bottom: 30px;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                height: 300px; 
            ">
                    <h4 style="font-size: 20px; font-weight: bold; margin-bottom: 5px;">ê³µë™ìˆ˜ê¸‰ â€¢ ì§€ì—­ì œí•œ</h4>
                    <hr style="border: 1px solid #e5e5e5; margin-top: 10px; margin-bottom: 10px;">
                    <div style="margin-bottom: 10px;">
                        <span style="font-size: 16px; font-weight: bold; color: #333;">ğŸ¤ ê³µë™ìˆ˜ê¸‰</span><br>
                        <span style="font-size: 18px; font-weight: 500; color: #000;">
                        {format_joint_contract(ê³µë™ìˆ˜ê¸‰)}</span>
                    </div>
                    <div>
                        <span style="font-size: 16px; font-weight: bold; color: #333;">ğŸ“ ì§€ì—­ì œí•œ</span><br>
                        <span style="font-size: 18px; font-weight: 500; color: #000;">
                            {ì°¸ê°€ê°€ëŠ¥ì§€ì—­ if ì§€ì—­ì œí•œ == 'Y' and pd.notna(ì°¸ê°€ê°€ëŠ¥ì§€ì—­) else 'ì—†ìŒ'}
                        </span>
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )          
            
        with col2:
            ì—…ì¢…ëª… = row.get('íˆ¬ì°°ê°€ëŠ¥ì—…ì¢…ëª…') or row.get('bidprcPsblIndstrytyNm')
            st.markdown(
                f"""
                <div style="
                background-color: #fff9e6; 
                border: 1px solid #e5e5e5;
                padding: 25px 20px; 
                border-radius: 15px; 
                margin-bottom: 30px;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                height: 300px; 
            ">
                    <h4 style="font-size: 20px; font-weight: bold; margin-bottom: 5px;">ğŸš«ì—…ì¢… ì œí•œ</h4>
                    <hr style="border: 1px solid #e5e5e5; margin-top: 10px; margin-bottom: 10px;">
                    <p style="font-size: 18px; font-weight: bold; overflow-y: auto; max-height: 90px;">
                        {"<br>".join([f"{i+1}. {item.strip()}" for i,
                                    item in enumerate(str(ì—…ì¢…ëª…).split(',')) if str(item).strip()]) 
                                    if ì—…ì¢…ëª… and str(ì—…ì¢…ëª…).strip() != "" else 'ê³µë¬¸ì„œì°¸ì¡°'}
                    </p>
                </div>
                """,
                unsafe_allow_html=True
            )

        with col3:
            st.markdown(
                f"""
                <div style="
                background-color: #f0f8ff; 
                border: 1px solid #e5e5e5;
                padding: 25px 20px; 
                border-radius: 15px; 
                margin-bottom: 30px;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                height: 300px; 
                ">
                    <h4 style="font-size: 20px; font-weight: bold; margin-bottom: 5px;">ğŸ’¡ ê¸°íƒ€ ì •ë³´</h4>
                    <hr style="border: 1px solid #e5e5e5; margin-top: 10px; margin-bottom: 10px;">
                    <p style="font-size: 16px;">
                        <strong>ê³µê³ ë²ˆí˜¸:</strong> {row.get('ê³µê³ ë²ˆí˜¸') or row.get('bidNtceNo', 'N/A')}<br>
                        <strong>ê³µê³ ê¸°ê´€:</strong> {row.get('ê³µê³ ê¸°ê´€') or row.get('ntceInsttNm', 'N/A')}<br>
                        <strong>ë¶„ë¥˜:</strong> {row.get('ë¶„ë¥˜') or row.get('bsnsDivNm', 'N/A')}
                    </p>
                </div>
                """,
                unsafe_allow_html=True
            )

        # GPT ìš”ì•½
        bid_no = row.get('ê³µê³ ë²ˆí˜¸') or row.get('bidNtceNo')
        summary_text, created_at, summary_type = get_bid_summary(bid_no)

        created_info = ""
        if created_at:
            type_label = "ìƒì„¸ë¬¸ì„œ ê¸°ë°˜" if summary_type == "hwp_based" else "ê¸°ë³¸ì •ë³´ ê¸°ë°˜"
            created_info = f" ({type_label}, ìƒì„±ì¼: {created_at})"

        st.markdown(
            f"""
            <div style="
                background-color: #f0f8ff; 
                border-left: 5px solid #4682b4; 
                padding: 15px;
                margin-top: 10px;
                border-radius: 10px;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            ">
            <div>
                <span style="font-size: 16px; font-weight: bold; color: #333;">AI ìƒì„¸ìš”ì•½{created_info}</span><br>   
                <hr style="border: 1px solid #e5e5e5; margin-top: 10px; margin-bottom: 10px;">         
            </div>
                <p style="font-size: 16px; font-weight: 500;">{summary_text}</p>
            </div>
            """, unsafe_allow_html=True
        )

# ========== ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ==========
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - Supabase ë²„ì „"""
    st.set_page_config(page_title="ì…ì°° ê³µê³  ì„œë¹„ìŠ¤ (Supabase)", layout="wide")

    # ì„¤ì • í™•ì¸
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        st.error("âš ï¸ Supabase ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. SUPABASE_URLê³¼ SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    if not OPENAI_API_KEY:
        st.error("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„ë² ë”© ë° GPT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    supabase = init_supabase_client()
    if not supabase:
        st.error("âŒ Supabase ì—°ê²° ì‹¤íŒ¨")
        st.stop()

    # ë°ì´í„° ë¡œë“œ
    df_live = get_live_bids_data()

    # í˜ì´ì§€ ìƒíƒœ ê´€ë¦¬
    page = st.session_state.get("page", "home")

    if page == "home":
        # íƒ­ ìƒì„±
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“¢ ì‹¤ì‹œê°„ ì…ì°° ê³µê³ ", "ğŸ” AI ê²€ìƒ‰", "ğŸš€ LangGraph AI ê²€ìƒ‰", "ğŸ¤– AI ë„ìš°ë¯¸"])
        
        with tab1:        
            show_live_bids_tab(df_live)
        
        with tab2:       
            show_semantic_search_tab()
            
        with tab3:
            add_langgraph_search_tab()
        
        with tab4:
            add_chatbot_to_streamlit()

    elif page == "detail":
        show_detail_page()

if __name__ == "__main__":
    main()